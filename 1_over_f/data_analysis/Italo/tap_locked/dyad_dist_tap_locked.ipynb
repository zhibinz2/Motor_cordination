{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import makefilter\n",
    "from scipy.signal import sosfiltfilt, hilbert\n",
    "from scipy.signal import savgol_filter\n",
    "import pickle\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def list_data_folders(directory_path,pattern):\n",
    "    \"\"\"\n",
    "    Lists all folders in the given directory that start with pattern.\n",
    "    \n",
    "    Parameters:\n",
    "    - directory_path: A string representing the path to the directory to search in.\n",
    "\n",
    "    - pattern: A string representing the pattern at the beginning of the folders' name.\n",
    "    \n",
    "    Returns:\n",
    "    - A list of folder names that meet the criteria.\n",
    "    \"\"\"\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(directory_path):\n",
    "        print(f\"The directory {directory_path} does not exist.\")\n",
    "        return []\n",
    "\n",
    "    # Get all items in the directory\n",
    "    all_items = os.listdir(directory_path)\n",
    "\n",
    "    # Filter for directories that start with pattern\n",
    "    folders_starting_with_pattern = [item for item in all_items\n",
    "                                if os.path.isdir(os.path.join(directory_path, item)) and item.startswith(pattern)]\n",
    "\n",
    "    return folders_starting_with_pattern\n",
    "\n",
    "def list_files_in_directory(directory_path):\n",
    "    \"\"\"\n",
    "    Lists all the files in the specified directory.\n",
    "    \n",
    "    Parameters:\n",
    "    - directory_path: A string representing the path to the directory to search in.\n",
    "    \n",
    "    Returns:\n",
    "    - A list of file names contained in the directory.\n",
    "    \"\"\"\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(directory_path):\n",
    "        print(f\"The directory {directory_path} does not exist.\")\n",
    "        return []\n",
    "\n",
    "    # Get all items in the directory\n",
    "    all_items = os.listdir(directory_path)\n",
    "\n",
    "    # Filter out only files\n",
    "    files_only = [item for item in all_items if os.path.isfile(os.path.join(directory_path, item))]\n",
    "\n",
    "    return files_only\n",
    "\n",
    "def open_mat_file(file_path):\n",
    "    \"\"\"\n",
    "    Opens a .mat file and returns its contents.\n",
    "    \n",
    "    Parameters:\n",
    "    - file_path: A string representing the path to the .mat file.\n",
    "    \n",
    "    Returns:\n",
    "    - A dictionary containing variables loaded from the .mat file.\n",
    "    \"\"\"\n",
    "    # Load the .mat file\n",
    "    data = loadmat(file_path)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def segment_time_series(data, segment_duration, overlap_duration, sampling_rate):\n",
    "    \"\"\"\n",
    "    Segments each time series in a 2D NumPy array into smaller time series segments \n",
    "    based on specified duration, overlap, and sampling rate.\n",
    "\n",
    "    Parameters:\n",
    "    - data (numpy.ndarray): A 2D NumPy array containing the time series data. Each \n",
    "    column represents a single time series.\n",
    "    - segment_duration (float): The duration of each segment in seconds. Determines\n",
    "     the length of each segment generated.\n",
    "    - overlap_duration (float): The duration of overlap between consecutive segments\n",
    "     in seconds. This specifies how much each segment should overlap with the next.\n",
    "    - sampling_rate (int): The number of samples per second in the time series data.\n",
    "     This is used to calculate the number of samples per segment and the overlap in \n",
    "     samples.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: A 3D NumPy array where the 1st dimension represents a segmented portion \n",
    "    of the original time series. The 2nd dimension in this array equals the number \n",
    "    of samples per segment, determined by the `segment_duration` and `sampling_rate`.\n",
    "    \"\"\"\n",
    "    # Calculate parameters\n",
    "    samples_per_segment = int(segment_duration * sampling_rate)\n",
    "    overlap_samples = int(overlap_duration * sampling_rate)\n",
    "    step_size = samples_per_segment - overlap_samples\n",
    "    \n",
    "    segments = []\n",
    "    len_data,_ = data.shape\n",
    "    for start in range(0, len_data - samples_per_segment + 1, step_size):\n",
    "        segment = data[start:start+samples_per_segment,:]\n",
    "        segments.append(segment)\n",
    "            \n",
    "    return np.array(segments)\n",
    "\n",
    "def save_to_pickle(dict_obj, file_path):\n",
    "    \"\"\"\n",
    "    Saves a given dictionary to a pickle file.\n",
    "\n",
    "    Parameters:\n",
    "    - dict_obj (dict): The dictionary to be saved.\n",
    "    - file_path (str): The path to the file where the dictionary will be saved.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    with open(file_path, 'wb') as file:\n",
    "        # Serialize the dictionary and save it to the file\n",
    "        pickle.dump(dict_obj, file)\n",
    "    print(f\"Data successfully saved to {file_path}.\")\n",
    "\n",
    "def corr_matrix_stack(corr_matrix_dic):\n",
    "    \"\"\"\n",
    "    Stacks correlation matrices along axis 0 and tracks their sizes and identifiers.\n",
    "\n",
    "    This function takes a dictionary of correlation matrices (2D NumPy arrays) and\n",
    "    stacks these matrices vertically. It also compiles a list of the sizes of these\n",
    "    matrices along axis 0, alongside their identifiers.\n",
    "\n",
    "    Parameters:\n",
    "    - corr_matrix_dic (dict): A dictionary where the keys are identifiers (e.g., file names)\n",
    "      and the values are correlation matrices (2D NumPy arrays). Each matrix is assumed to\n",
    "      have the same number of columns but can vary in the number of rows.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: A single 2D NumPy array resulting from stacking all the input matrices\n",
    "      along axis 0.\n",
    "    - list of tuples: Each tuple contains an identifier (key from the input dictionary) and\n",
    "      an integer representing the size (number of rows) of the corresponding matrix before\n",
    "      stacking. This list maintains the order in which matrices were stacked.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize a list to keep track of each matrix's identifier and its number of rows\n",
    "    size_matrices = []\n",
    "    \n",
    "    # Initialize a list to hold all matrices for concatenation\n",
    "    matrix_list = []\n",
    "    \n",
    "    # Iterate over the dictionary items\n",
    "    for file, matrix in corr_matrix_dic.items():\n",
    "        # Append each matrix to the list for later concatenation\n",
    "        matrix_list.append(matrix)\n",
    "        # Append a tuple of the matrix's identifier and its number of rows to the tracking list\n",
    "        size_matrices.append((file, matrix.shape[0]))\n",
    "    \n",
    "    # Concatenate all matrices vertically\n",
    "    stacked_array = np.concatenate(matrix_list, axis=0)\n",
    "    \n",
    "    # Return the stacked array and the list of identifiers with their corresponding matrix sizes\n",
    "    return stacked_array, size_matrices\n",
    "\n",
    "def corr_dist(A,B):\n",
    "    from scipy.linalg import norm\n",
    "    D = np.transpose(np.conj(A))@B\n",
    "    dist = np.real(np.log(1.0/(np.trace(D)/(norm(A)*norm(B)))))\n",
    "    return dist\n",
    "\n",
    "def sort_key(filename):\n",
    "    trial_part = int(filename.split('_')[-1].split('.')[0])  # Extract the trial number\n",
    "    return trial_part\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_time_series_taplocked(data, segment_duration, sampling_rate, tr_samples):\n",
    "    \"\"\"\n",
    "    Segments each time series in a 2D NumPy array into smaller time series segments \n",
    "    based on specified duration, and sampling rate.\n",
    "\n",
    "    Parameters:\n",
    "    - data (numpy.ndarray): A 2D NumPy array containing the time series data. Each \n",
    "    column represents a single time series.\n",
    "    - segment_duration (float): The duration of each segment in seconds. Determines\n",
    "     the length of each segment generated.\n",
    "    - sampling_rate (int): The number of samples per second in the time series data.\n",
    "     This is used to calculate the number of samples per segment and the overlap in \n",
    "     samples.\n",
    "    - tr_samples: index of taps in the samples of a single trial\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: A 3D NumPy array where the 1st dimension represents a segmented portion \n",
    "    of the original time series. The 2nd dimension in this array equals the number \n",
    "    of samples per segment, determined by the `segment_duration` and `sampling_rate`.\n",
    "    \"\"\"\n",
    "    # Calculate parameters\n",
    "    samples_per_segment = int(segment_duration * sampling_rate)\n",
    "\n",
    "    segments = []\n",
    "    for start in tr_samples:\n",
    "        segment = data[start-samples_per_segment:start,:]\n",
    "        segments.append(segment)\n",
    "\n",
    "    return np.array(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory_path = '/ssd/zhibin/1overf/Cleaned_sourcedata/cortical_source_data/'  # Replace this with the path to your directory\n",
    "# folders = list_data_folders(directory_path,'20')\n",
    "\n",
    "#Butterworth filters.\n",
    "butt_filter1,butt_w,butt_h = makefilter.makefiltersos(2000,50,60)\n",
    "# window_len = 2.0\n",
    "window_len = 0.5\n",
    "# overlap = 0.5\n",
    "# overlap_len = overlap*window_len\n",
    "butt_filter2,butt_w,butt_h = makefilter.makefiltersos(2000,1.0/window_len,0.5/window_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = ['20220713','20220721',\n",
    "           '20220804','20220808',\n",
    "           '20220810','20220811',\n",
    "           '20220815','20220816',\n",
    "           '20221003','2022100401',\n",
    "           '2022100402','20221005']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder=folders[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20220713\n"
     ]
    }
   ],
   "source": [
    "print(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_dic = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = '/ssd/zhibin/1overf/Cleaned_sourcedata/cortical_source_data/' + folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/ssd/zhibin/1overf/Cleaned_sourcedata/cortical_source_data/20220713'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = list_files_in_directory(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['subj1_tr_8.mat',\n",
       " 'subj1_tr_4.mat',\n",
       " 'subj2_tr_4.mat',\n",
       " 'subj2_tr_9.mat',\n",
       " 'subj1_tr_11.mat',\n",
       " 'subj1_tr_5.mat',\n",
       " 'subj1_tr_1.mat',\n",
       " 'subj1_tr_10.mat',\n",
       " 'subj1_tr_3.mat',\n",
       " 'subj2_tr_2.mat',\n",
       " 'subj2_tr_1.mat',\n",
       " 'subj1_tr_6.mat',\n",
       " 'subj1_tr_9.mat',\n",
       " 'subj2_tr_6.mat',\n",
       " 'subj2_tr_3.mat',\n",
       " 'subj2_tr_12.mat',\n",
       " 'subj1_tr_7.mat',\n",
       " 'subj2_tr_8.mat',\n",
       " 'subj2_tr_11.mat',\n",
       " 'subj1_tr_2.mat',\n",
       " 'subj2_tr_7.mat',\n",
       " 'subj2_tr_5.mat',\n",
       " 'subj1_tr_12.mat',\n",
       " 'subj2_tr_10.mat']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load event time points\n",
    "beha_filename = '/home/zhibinz2/Documents/GitHub/finger_tapping_behavioral_data/clean_' + str(folder) + '_bpchan.mat'\n",
    "beh_data =  loadmat(beha_filename)\n",
    "samples = beh_data['samples'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute difference(error) of the samples\n",
    "tr=0\n",
    "tr_samples = samples[tr]\n",
    "num_tp=len(tr_samples)\n",
    "tr_error=np.zeros(num_tp)\n",
    "for tp in range(num_tp):\n",
    "    tr_error[tp]=np.abs(tr_samples[tp][0]-tr_samples[tp][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files listed.\n"
     ]
    }
   ],
   "source": [
    "for filename in files:\n",
    "    subject, rest = filename.split('_', 1)\n",
    "    if subject not in files_dic:\n",
    "        files_dic[subject] = {}\n",
    "    file = folder + '/' + filename\n",
    "    files_dic[subject][filename] = directory_path + '/' + filename\n",
    "print('files listed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['subj1_tr_8.mat', 'subj1_tr_4.mat', 'subj1_tr_11.mat', 'subj1_tr_5.mat', 'subj1_tr_1.mat', 'subj1_tr_10.mat', 'subj1_tr_3.mat', 'subj1_tr_6.mat', 'subj1_tr_9.mat', 'subj1_tr_7.mat', 'subj1_tr_2.mat', 'subj1_tr_12.mat'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_dic['subj1'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['subj2_tr_4.mat', 'subj2_tr_9.mat', 'subj2_tr_2.mat', 'subj2_tr_1.mat', 'subj2_tr_6.mat', 'subj2_tr_3.mat', 'subj2_tr_12.mat', 'subj2_tr_8.mat', 'subj2_tr_11.mat', 'subj2_tr_7.mat', 'subj2_tr_5.mat', 'subj2_tr_10.mat'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_dic['subj2'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_matrices = {}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "subject 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject='subj1'\n",
    "subj=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subj1\n"
     ]
    }
   ],
   "source": [
    "print(subject) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrices = {}\n",
    "subject_files = list(files_dic[subject].keys())\n",
    "subject_files = sorted(subject_files,key=sort_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['subj1_tr_1.mat',\n",
       " 'subj1_tr_2.mat',\n",
       " 'subj1_tr_3.mat',\n",
       " 'subj1_tr_4.mat',\n",
       " 'subj1_tr_5.mat',\n",
       " 'subj1_tr_6.mat',\n",
       " 'subj1_tr_7.mat',\n",
       " 'subj1_tr_8.mat',\n",
       " 'subj1_tr_9.mat',\n",
       " 'subj1_tr_10.mat',\n",
       " 'subj1_tr_11.mat',\n",
       " 'subj1_tr_12.mat']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20220713/subj1_tr_1.mat'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr=1\n",
    "file='subj1_tr_1.mat'\n",
    "path = files_dic[subject][file]\n",
    "filename = folder + '/' + file\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/ssd/zhibin/1overf/Cleaned_sourcedata/cortical_source_data/20220713/subj1_tr_1.mat'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the data from a sample file.\n",
    "data = open_mat_file(path)\n",
    "#Get signal, filter and downsample.\n",
    "signal = data['agr_source_data']\n",
    "filtered_signal = sosfiltfilt(butt_filter1, signal, axis=0)\n",
    "filtered_signal = sosfiltfilt(butt_filter2, filtered_signal, axis=0)\n",
    "# downsampled_signal = filtered_signal[::10,:]\n",
    "# analytic_signal = hilbert(downsampled_signal,axis=0)\n",
    "# no downsampling\n",
    "analytic_signal = hilbert(filtered_signal,axis=0)\n",
    "sg = int(np.floor(100/(25))*2+1)\n",
    "ts1 = savgol_filter(np.real(analytic_signal),sg,1,axis = 0,mode = 'interp')\n",
    "ts2 = savgol_filter(np.imag(analytic_signal),sg,1,axis = 0,mode = 'interp')\n",
    "ts3 = ts1+1j*ts2\n",
    "\n",
    "# 1m 8s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3331531/3717702527.py:29: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(segments)\n"
     ]
    }
   ],
   "source": [
    "# time_windows = segment_time_series(ts3, window_len, overlap_len, 200)\n",
    "# segment data with tap-locked windows\n",
    "tr_samples_subj = samples[tr][:,subj]\n",
    "time_windows = segment_time_series_taplocked(ts3, window_len, 2000, tr_samples_subj)\n",
    "# 50s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 448)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(time_windows[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhibinz2/anaconda3/envs/eeg/lib/python3.9/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/home/zhibinz2/anaconda3/envs/eeg/lib/python3.9/site-packages/numpy/core/_methods.py:182: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n",
      "/home/zhibinz2/anaconda3/envs/eeg/lib/python3.9/site-packages/numpy/lib/function_base.py:2845: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  c = cov(x, y, rowvar, dtype=dtype)\n",
      "/home/zhibinz2/anaconda3/envs/eeg/lib/python3.9/site-packages/numpy/lib/function_base.py:2704: RuntimeWarning: divide by zero encountered in divide\n",
      "  c *= np.true_divide(1, fact)\n",
      "/home/zhibinz2/anaconda3/envs/eeg/lib/python3.9/site-packages/numpy/lib/function_base.py:2704: RuntimeWarning: invalid value encountered in multiply\n",
      "  c *= np.true_divide(1, fact)\n",
      "/home/zhibinz2/anaconda3/envs/eeg/lib/python3.9/site-packages/numpy/lib/function_base.py:2853: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "/home/zhibinz2/anaconda3/envs/eeg/lib/python3.9/site-packages/numpy/lib/function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    }
   ],
   "source": [
    "corr_matrix = []\n",
    "for window in time_windows:\n",
    "    corr_matrix.append(np.corrcoef(window, rowvar=False))\n",
    "corr_matrix = np.array(corr_matrix)\n",
    "corr_matrix = corr_matrix - np.mean(corr_matrix,axis=0)\n",
    "corr_matrices[filename] = corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "subj1_tr_1.mat\n",
      "1\n",
      "subj1_tr_2.mat\n",
      "2\n",
      "subj1_tr_3.mat\n",
      "3\n",
      "subj1_tr_4.mat\n",
      "4\n",
      "subj1_tr_5.mat\n",
      "5\n",
      "subj1_tr_6.mat\n",
      "6\n",
      "subj1_tr_7.mat\n",
      "7\n",
      "subj1_tr_8.mat\n",
      "8\n",
      "subj1_tr_9.mat\n",
      "9\n",
      "subj1_tr_10.mat\n",
      "10\n",
      "subj1_tr_11.mat\n",
      "11\n",
      "subj1_tr_12.mat\n",
      "correlation matrices calculated.\n"
     ]
    }
   ],
   "source": [
    "for tr, file in enumerate(subject_files):\n",
    "    print(tr)\n",
    "    print(file)\n",
    "    path = files_dic[subject][file]\n",
    "    filename = folder + '/' + file\n",
    "    # print(path)\n",
    "    #Import the data from a sample file.\n",
    "    data = open_mat_file(path)\n",
    "    #Get signal, filter and downsample.\n",
    "    signal = data['agr_source_data']\n",
    "    filtered_signal = sosfiltfilt(butt_filter1, signal, axis=0)\n",
    "    filtered_signal = sosfiltfilt(butt_filter2, filtered_signal, axis=0)\n",
    "    # downsampled_signal = filtered_signal[::10,:]\n",
    "    # analytic_signal = hilbert(downsampled_signal,axis=0)\n",
    "    # no downsampling\n",
    "    analytic_signal = hilbert(filtered_signal,axis=0)\n",
    "    sg = int(np.floor(100/(25))*2+1)\n",
    "    ts1 = savgol_filter(np.real(analytic_signal),sg,1,axis = 0,mode = 'interp')\n",
    "    ts2 = savgol_filter(np.imag(analytic_signal),sg,1,axis = 0,mode = 'interp')\n",
    "    ts3 = ts1+1j*ts2\n",
    "    \n",
    "    # time_windows = segment_time_series(ts3, window_len, overlap_len, 200)\n",
    "    # segment data with tap-locked windows\n",
    "    tr_samples = samples[tr][:,subj]\n",
    "    time_windows = segment_time_series_taplocked(ts3, window_len, 2000, tr_samples)\n",
    "\n",
    "    corr_matrix = []\n",
    "    for window in time_windows:\n",
    "        corr_matrix.append(np.corrcoef(window, rowvar=False))\n",
    "    corr_matrix = np.array(corr_matrix)\n",
    "    corr_matrix = corr_matrix - np.mean(corr_matrix,axis=0)\n",
    "    corr_matrices[filename] = corr_matrix\n",
    "print('correlation matrices calculated.')\n",
    "# 9 min for 12 files in one subject\n",
    "# 9 min x 2 subjects x 12 session = 3.6 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix,sizes = corr_matrix_stack(corr_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['20220713/subj1_tr_1.mat', '20220713/subj1_tr_2.mat', '20220713/subj1_tr_3.mat', '20220713/subj1_tr_4.mat', '20220713/subj1_tr_5.mat', '20220713/subj1_tr_6.mat', '20220713/subj1_tr_7.mat', '20220713/subj1_tr_8.mat', '20220713/subj1_tr_9.mat', '20220713/subj1_tr_10.mat', '20220713/subj1_tr_11.mat', '20220713/subj1_tr_12.mat'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_matrices.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('20220713/subj1_tr_1.mat', 128),\n",
       " ('20220713/subj1_tr_2.mat', 148),\n",
       " ('20220713/subj1_tr_3.mat', 124),\n",
       " ('20220713/subj1_tr_4.mat', 116),\n",
       " ('20220713/subj1_tr_5.mat', 145),\n",
       " ('20220713/subj1_tr_6.mat', 146),\n",
       " ('20220713/subj1_tr_7.mat', 114),\n",
       " ('20220713/subj1_tr_8.mat', 141),\n",
       " ('20220713/subj1_tr_9.mat', 132),\n",
       " ('20220713/subj1_tr_10.mat', 140),\n",
       " ('20220713/subj1_tr_11.mat', 145),\n",
       " ('20220713/subj1_tr_12.mat', 147)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1626, 448, 448)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1626"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_matrix = np.zeros((len(matrix),len(matrix)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in combinations(range(len(matrix)),2):\n",
    "    print(i)\n",
    "    matrix1 = matrix[i,:,:]\n",
    "    matrix2 = matrix[j,:,:]\n",
    "    dist_matrix[i,j] = corr_dist(matrix1,matrix2)\n",
    "    dist_matrix[j,i] = dist_matrix[i,j]\n",
    "        \n",
    "print('distance matrices calculated.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_matrices[subject] = {'distances': dist_matrix, 'sizes':sizes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_matrices.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_matrices['subj1'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(subject_matrices['subj1']['distances'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_matrices['subj1']['sizes']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdf5storage import savemat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in folders:\n",
    "    print(folder)\n",
    "    files_dic = {}\n",
    "#    for folder in folders:\n",
    "#    folder_dic = {}\n",
    "    directory_path = '/ssd/zhibin/1overf/Cleaned_sourcedata/cortical_source_data/' + folder\n",
    "    files = list_files_in_directory(directory_path)\n",
    "\n",
    "    # load event time points\n",
    "    beha_filename = '/home/zhibinz2/Documents/GitHub/finger_tapping_behavioral_data/clean_' + str(folder) + '_bpchan.mat'\n",
    "    beh_data = loadmat(beha_filename)\n",
    "    samples = beh_data['samples'][0]\n",
    "\n",
    "    for filename in files:\n",
    "        subject, rest = filename.split('_', 1)\n",
    "        if subject not in files_dic:\n",
    "            files_dic[subject] = {}\n",
    "        file = folder + '/' + filename\n",
    "        files_dic[subject][filename] = directory_path + '/' + filename\n",
    "    print('files listed.')\n",
    "\n",
    "    subject_matrices = {}\n",
    "    for subj,subject in enumerate(['subj1','subj2']):\n",
    "        print(subj)\n",
    "        print(subject) \n",
    "        corr_matrices = {}\n",
    "        subject_files = list(files_dic[subject].keys())\n",
    "        subject_files = sorted(subject_files,key=sort_key)\n",
    "        \n",
    "\n",
    "        for tr, file in enumerate(subject_files):\n",
    "            print(tr)\n",
    "            print(file)\n",
    "            path = files_dic[subject][file]\n",
    "            filename = folder + '/' + file\n",
    "            # print(path)\n",
    "\n",
    "            #Import the data from a sample file.\n",
    "            data = open_mat_file(path)\n",
    "\n",
    "            #Get signal, filter and downsample.\n",
    "            signal = data['agr_source_data']\n",
    "            filtered_signal = sosfiltfilt(butt_filter1, signal, axis=0)\n",
    "            filtered_signal = sosfiltfilt(butt_filter2, filtered_signal, axis=0)\n",
    "            # downsampled_signal = filtered_signal[::10,:]\n",
    "            # analytic_signal = hilbert(downsampled_signal,axis=0)\n",
    "            # no downsampling\n",
    "            analytic_signal = hilbert(filtered_signal,axis=0)\n",
    "\n",
    "            sg = int(np.floor(100/(25))*2+1)\n",
    "            ts1 = savgol_filter(np.real(analytic_signal),sg,1,axis = 0,mode = 'interp')\n",
    "            ts2 = savgol_filter(np.imag(analytic_signal),sg,1,axis = 0,mode = 'interp')\n",
    "            ts3 = ts1+1j*ts2\n",
    "            \n",
    "            # time_windows = segment_time_series(ts3, window_len, overlap_len, 200)\n",
    "            # segment data with tap-locked windows\n",
    "            tr_samples = samples[tr][:,subj]\n",
    "            time_windows = segment_time_series_taplocked(ts3, window_len, 2000,tr_samples)\n",
    "            \n",
    "            corr_matrix = []\n",
    "            for window in time_windows:\n",
    "                corr_matrix.append(np.corrcoef(window, rowvar=False))\n",
    "            corr_matrix = np.array(corr_matrix)\n",
    "            corr_matrix = corr_matrix - np.mean(corr_matrix,axis=0) # from one file\n",
    "            corr_matrices[filename] = corr_matrix # from all 12 files in a folder\n",
    "        print('correlation matrices calculated.')\n",
    "\n",
    "        # matrix,sizes = corr_matrix_stack(corr_matrices) # from all 12 files in a folder\n",
    "        # dist_matrix = np.zeros((len(matrix),len(matrix)))\n",
    "        # for i,j in combinations(range(len(matrix)),2):\n",
    "        #     print(i)\n",
    "        #     matrix1 = matrix[i,:,:]\n",
    "        #     matrix2 = matrix[j,:,:]\n",
    "        #     dist_matrix[i,j] = corr_dist(matrix1,matrix2)\n",
    "        #     dist_matrix[j,i] = dist_matrix[i,j]\n",
    "        \n",
    "        # print('distance matrices calculated.')\n",
    "        # subject_matrices[subject] = {'matrix': matrix, 'sizes':sizes}\n",
    "        subject_matrices[subject] = {'corr_matrices': corr_matrices}\n",
    "    \n",
    "    # filename_save = '/ssd/zhibin/1overf/Cleaned_sourcedata/cortical_source_data/Italo/correlation_distances/dyad_' + folder + '_distances.pkl'\n",
    "    # filename_save = '/ssd/zhibin/1overf/Cleaned_sourcedata/cortical_source_data/taplocked/correlation_distances/dyad_' + folder + '_distances.pkl'\n",
    "    # print(filename_save)\n",
    "    # save_to_pickle(subject_matrices, filename_save)\n",
    "\n",
    "    filename_save = '/ssd/zhibin/1overf/Cleaned_sourcedata/cortical_source_data/taplocked/corr_matrices/dyad_' + folder + '_corrmat.pkl'\n",
    "    print(filename_save)\n",
    "    save_to_pickle(subject_matrices, filename_save)\n",
    "    # savemat('dyad_' + folder + '_corrmat.mat', subject_matrices)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save distance mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in folders:\n",
    "    print(folder)\n",
    "    files_dic = {}\n",
    "#    for folder in folders:\n",
    "#    folder_dic = {}\n",
    "    directory_path = '/ssd/zhibin/1overf/Cleaned_sourcedata/cortical_source_data/' + folder\n",
    "    files = list_files_in_directory(directory_path)\n",
    "\n",
    "    # load event time points\n",
    "    beha_filename = '/home/zhibinz2/Documents/GitHub/finger_tapping_behavioral_data/clean_' + str(folder) + '_bpchan.mat'\n",
    "    beh_data =  loadmat(beha_filename)\n",
    "    samples = beh_data['samples'][0]\n",
    "\n",
    "    for filename in files:\n",
    "        subject, rest = filename.split('_', 1)\n",
    "        if subject not in files_dic:\n",
    "            files_dic[subject] = {}\n",
    "        file = folder + '/' + filename\n",
    "        files_dic[subject][filename] = directory_path + '/' + filename\n",
    "    print('files listed.')\n",
    "\n",
    "    subject_matrices = {}\n",
    "    for subj,subject in enumerate(['subj1','subj2']):\n",
    "        print(subj)\n",
    "        print(subject) \n",
    "        corr_matrices = {}\n",
    "        subject_files = list(files_dic[subject].keys())\n",
    "        subject_files = sorted(subject_files,key=sort_key)\n",
    "        \n",
    "\n",
    "        for tr, file in enumerate(subject_files):\n",
    "            print(tr)\n",
    "            print(file)\n",
    "            path = files_dic[subject][file]\n",
    "            filename = folder + '/' + file\n",
    "            # print(path)\n",
    "\n",
    "            #Import the data from a sample file.\n",
    "            data = open_mat_file(path)\n",
    "\n",
    "            #Get signal, filter and downsample.\n",
    "            signal = data['agr_source_data']\n",
    "            filtered_signal = sosfiltfilt(butt_filter1, signal, axis=0)\n",
    "            filtered_signal = sosfiltfilt(butt_filter2, filtered_signal, axis=0)\n",
    "            # downsampled_signal = filtered_signal[::10,:]\n",
    "            # analytic_signal = hilbert(downsampled_signal,axis=0)\n",
    "            # no downsampling\n",
    "            analytic_signal = hilbert(filtered_signal,axis=0)\n",
    "\n",
    "            sg = int(np.floor(100/(25))*2+1)\n",
    "            ts1 = savgol_filter(np.real(analytic_signal),sg,1,axis = 0,mode = 'interp')\n",
    "            ts2 = savgol_filter(np.imag(analytic_signal),sg,1,axis = 0,mode = 'interp')\n",
    "            ts3 = ts1+1j*ts2\n",
    "            \n",
    "            # time_windows = segment_time_series(ts3, window_len, overlap_len, 200)\n",
    "            # segment data with tap-locked windows\n",
    "            tr_samples = samples[tr][:,subj]\n",
    "            time_windows = segment_time_series_taplocked(ts3, window_len, 2000,tr_samples)\n",
    "            \n",
    "            corr_matrix = []\n",
    "            for window in time_windows:\n",
    "                corr_matrix.append(np.corrcoef(window, rowvar=False))\n",
    "            corr_matrix = np.array(corr_matrix)\n",
    "            corr_matrix = corr_matrix - np.mean(corr_matrix,axis=0) # from one file\n",
    "            corr_matrices[filename] = corr_matrix # from all 12 files in a folder\n",
    "        print('correlation matrices calculated.')\n",
    "\n",
    "        matrix,sizes = corr_matrix_stack(corr_matrices) # from all 12 files in a folder\n",
    "        dist_matrix = np.zeros((len(matrix),len(matrix)))\n",
    "        for i,j in combinations(range(len(matrix)),2):\n",
    "            print(i)\n",
    "            matrix1 = matrix[i,:,:]\n",
    "            matrix2 = matrix[j,:,:]\n",
    "            dist_matrix[i,j] = corr_dist(matrix1,matrix2)\n",
    "            dist_matrix[j,i] = dist_matrix[i,j]\n",
    "        \n",
    "        print('distance matrices calculated.')\n",
    "        subject_matrices[subject] = {'distances': dist_matrix, 'sizes':sizes}\n",
    "    \n",
    "    # filename_save = '/ssd/zhibin/1overf/Cleaned_sourcedata/cortical_source_data/Italo/correlation_distances/dyad_' + folder + '_distances.pkl'\n",
    "    filename_save = '/ssd/zhibin/1overf/Cleaned_sourcedata/cortical_source_data/taplocked/correlation_distances/dyad_' + folder + '_distances.pkl'\n",
    "    print(filename_save)\n",
    "    save_to_pickle(subject_matrices, filename_save)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open_mat_file(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compute and save errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute and save errors\n",
    "errors={}\n",
    "for folder in folders:\n",
    "    errors_folder={}   \n",
    "    print(folder)\n",
    "    files_dic = {}\n",
    "#    for folder in folders:\n",
    "#    folder_dic = {}\n",
    "    directory_path = '/ssd/zhibin/1overf/Cleaned_sourcedata/cortical_source_data/' + folder\n",
    "    files = list_files_in_directory(directory_path)\n",
    "\n",
    "    # load event time points\n",
    "    beha_filename = '/home/zhibinz2/Documents/GitHub/finger_tapping_behavioral_data/clean_' + str(folder) + '_bpchan.mat'\n",
    "    beh_data =  loadmat(beha_filename)\n",
    "    samples = beh_data['samples'][0]\n",
    "\n",
    "    for filename in files:\n",
    "        subject, rest = filename.split('_', 1)\n",
    "        if subject not in files_dic:\n",
    "            files_dic[subject] = {}\n",
    "        file = folder + '/' + filename\n",
    "        files_dic[subject][filename] = directory_path + '/' + filename\n",
    "    print('files listed.')\n",
    "\n",
    "    for subj,subject in enumerate(['subj1','subj2']):\n",
    "        # print(subj)\n",
    "        print(subject) \n",
    "        corr_matrices = {}\n",
    "        subject_files = list(files_dic[subject].keys())\n",
    "        subject_files = sorted(subject_files,key=sort_key) \n",
    "\n",
    "        for tr, file in enumerate(subject_files):\n",
    "            # print(tr)\n",
    "            print(file)\n",
    "            path = files_dic[subject][file]\n",
    "            filename = folder + '/' + file\n",
    "            # print(path)\n",
    "\n",
    "            # time_windows = segment_time_series(ts3, window_len, overlap_len, 200)\n",
    "            # segment data with tap-locked windows\n",
    "            tr_samples = samples[tr][:,subj]\n",
    "            time_windows = segment_time_series_taplocked(ts3, window_len, 2000,tr_samples)\n",
    "\n",
    "            # Compute difference(error) of the samples\n",
    "            if subject == 'subj1':\n",
    "                tr_samples_both = samples[tr]\n",
    "                num_tp=len(tr_samples_both)\n",
    "                tr_error=np.zeros(num_tp)\n",
    "                for tp in range(num_tp):\n",
    "                    tr_error[tp]=np.abs(tr_samples_both[tp][0]-tr_samples_both[tp][1])\n",
    "                errors_folder[file.split('_')[1]+(file.split('_')[2]).split('.')[0]]=tr_error\n",
    "    \n",
    "    errors[folder]=errors_folder\n",
    "                    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_folder.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_folder['tr1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors['20220713']['tr1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdf5storage import loadmat, savemat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors[folder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdf5storage\n",
    "\n",
    "ses12_errors=[]\n",
    "\n",
    "for folder in folders:\n",
    "    print(folder)\n",
    "    ses12_errors.append(errors[folder])\n",
    "\n",
    "ses12_errors.append(folders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert the list to an object array (equivalent to MATLAB cell array)\n",
    "ses12_errors_cell_array = np.empty(len(ses12_errors), dtype=object)\n",
    "# Fill the object array with the data\n",
    "for idx, arr in enumerate(ses12_errors):\n",
    "    ses12_errors_cell_array[idx] = arr\n",
    "# Save the object array (cell-like structure) to a .mat file\n",
    "hdf5storage.savemat('ses12_errors.mat', {'cell_data': ses12_errors_cell_array}, format='7.3')\n",
    "    \n",
    "# 30s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savemat('errors.mat',errors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdict=loadmat('errors.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdict['20220713']['tr1']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
