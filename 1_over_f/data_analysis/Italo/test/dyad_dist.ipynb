{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import makefilter\n",
    "from scipy.signal import sosfiltfilt, hilbert\n",
    "from scipy.signal import savgol_filter\n",
    "import pickle\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def list_data_folders(directory_path,pattern):\n",
    "    \"\"\"\n",
    "    Lists all folders in the given directory that start with pattern.\n",
    "    \n",
    "    Parameters:\n",
    "    - directory_path: A string representing the path to the directory to search in.\n",
    "\n",
    "    - pattern: A string representing the pattern at the beginning of the folders' name.\n",
    "    \n",
    "    Returns:\n",
    "    - A list of folder names that meet the criteria.\n",
    "    \"\"\"\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(directory_path):\n",
    "        print(f\"The directory {directory_path} does not exist.\")\n",
    "        return []\n",
    "\n",
    "    # Get all items in the directory\n",
    "    all_items = os.listdir(directory_path)\n",
    "\n",
    "    # Filter for directories that start with pattern\n",
    "    folders_starting_with_pattern = [item for item in all_items\n",
    "                                if os.path.isdir(os.path.join(directory_path, item)) and item.startswith(pattern)]\n",
    "\n",
    "    return folders_starting_with_pattern\n",
    "\n",
    "def list_files_in_directory(directory_path):\n",
    "    \"\"\"\n",
    "    Lists all the files in the specified directory.\n",
    "    \n",
    "    Parameters:\n",
    "    - directory_path: A string representing the path to the directory to search in.\n",
    "    \n",
    "    Returns:\n",
    "    - A list of file names contained in the directory.\n",
    "    \"\"\"\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(directory_path):\n",
    "        print(f\"The directory {directory_path} does not exist.\")\n",
    "        return []\n",
    "\n",
    "    # Get all items in the directory\n",
    "    all_items = os.listdir(directory_path)\n",
    "\n",
    "    # Filter out only files\n",
    "    files_only = [item for item in all_items if os.path.isfile(os.path.join(directory_path, item))]\n",
    "\n",
    "    return files_only\n",
    "\n",
    "def open_mat_file(file_path):\n",
    "    \"\"\"\n",
    "    Opens a .mat file and returns its contents.\n",
    "    \n",
    "    Parameters:\n",
    "    - file_path: A string representing the path to the .mat file.\n",
    "    \n",
    "    Returns:\n",
    "    - A dictionary containing variables loaded from the .mat file.\n",
    "    \"\"\"\n",
    "    # Load the .mat file\n",
    "    data = loadmat(file_path)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def segment_time_series(data, segment_duration, overlap_duration, sampling_rate):\n",
    "    \"\"\"\n",
    "    Segments each time series in a 2D NumPy array into smaller time series segments \n",
    "    based on specified duration, overlap, and sampling rate.\n",
    "\n",
    "    Parameters:\n",
    "    - data (numpy.ndarray): A 2D NumPy array containing the time series data. Each \n",
    "    column represents a single time series.\n",
    "    - segment_duration (float): The duration of each segment in seconds. Determines\n",
    "     the length of each segment generated.\n",
    "    - overlap_duration (float): The duration of overlap between consecutive segments\n",
    "     in seconds. This specifies how much each segment should overlap with the next.\n",
    "    - sampling_rate (int): The number of samples per second in the time series data.\n",
    "     This is used to calculate the number of samples per segment and the overlap in \n",
    "     samples.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: A 3D NumPy array where the 1st dimension represents a segmented portion \n",
    "    of the original time series. The 2nd dimension in this array equals the number \n",
    "    of samples per segment, determined by the `segment_duration` and `sampling_rate`.\n",
    "    \"\"\"\n",
    "    # Calculate parameters\n",
    "    samples_per_segment = int(segment_duration * sampling_rate)\n",
    "    overlap_samples = int(overlap_duration * sampling_rate)\n",
    "    step_size = samples_per_segment - overlap_samples\n",
    "    \n",
    "    segments = []\n",
    "    len_data,_ = data.shape\n",
    "    for start in range(0, len_data - samples_per_segment + 1, step_size):\n",
    "        segment = data[start:start+samples_per_segment,:]\n",
    "        segments.append(segment)\n",
    "            \n",
    "    return np.array(segments)\n",
    "\n",
    "def save_to_pickle(dict_obj, file_path):\n",
    "    \"\"\"\n",
    "    Saves a given dictionary to a pickle file.\n",
    "\n",
    "    Parameters:\n",
    "    - dict_obj (dict): The dictionary to be saved.\n",
    "    - file_path (str): The path to the file where the dictionary will be saved.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    with open(file_path, 'wb') as file:\n",
    "        # Serialize the dictionary and save it to the file\n",
    "        pickle.dump(dict_obj, file)\n",
    "    print(f\"Data successfully saved to {file_path}.\")\n",
    "\n",
    "def corr_matrix_stack(corr_matrix_dic):\n",
    "    \"\"\"\n",
    "    Stacks correlation matrices along axis 0 and tracks their sizes and identifiers.\n",
    "\n",
    "    This function takes a dictionary of correlation matrices (2D NumPy arrays) and\n",
    "    stacks these matrices vertically. It also compiles a list of the sizes of these\n",
    "    matrices along axis 0, alongside their identifiers.\n",
    "\n",
    "    Parameters:\n",
    "    - corr_matrix_dic (dict): A dictionary where the keys are identifiers (e.g., file names)\n",
    "      and the values are correlation matrices (2D NumPy arrays). Each matrix is assumed to\n",
    "      have the same number of columns but can vary in the number of rows.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: A single 2D NumPy array resulting from stacking all the input matrices\n",
    "      along axis 0.\n",
    "    - list of tuples: Each tuple contains an identifier (key from the input dictionary) and\n",
    "      an integer representing the size (number of rows) of the corresponding matrix before\n",
    "      stacking. This list maintains the order in which matrices were stacked.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize a list to keep track of each matrix's identifier and its number of rows\n",
    "    size_matrices = []\n",
    "    \n",
    "    # Initialize a list to hold all matrices for concatenation\n",
    "    matrix_list = []\n",
    "    \n",
    "    # Iterate over the dictionary items\n",
    "    for file, matrix in corr_matrix_dic.items():\n",
    "        # Append each matrix to the list for later concatenation\n",
    "        matrix_list.append(matrix)\n",
    "        # Append a tuple of the matrix's identifier and its number of rows to the tracking list\n",
    "        size_matrices.append((file, matrix.shape[0]))\n",
    "    \n",
    "    # Concatenate all matrices vertically\n",
    "    stacked_array = np.concatenate(matrix_list, axis=0)\n",
    "    \n",
    "    # Return the stacked array and the list of identifiers with their corresponding matrix sizes\n",
    "    return stacked_array, size_matrices\n",
    "\n",
    "def corr_dist(A,B):\n",
    "    from scipy.linalg import norm\n",
    "    D = np.transpose(np.conj(A))@B\n",
    "    dist = np.real(np.log(1.0/(np.trace(D)/(norm(A)*norm(B)))))\n",
    "    return dist\n",
    "\n",
    "def sort_key(filename):\n",
    "    trial_part = int(filename.split('_')[-1].split('.')[0])  # Extract the trial number\n",
    "    return trial_part\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory_path = '/ssd/zhibin/1overf/Cleaned_sourcedata/cortical_source_data/'  # Replace this with the path to your directory\n",
    "# folders = list_data_folders(directory_path,'20')\n",
    "\n",
    "#Butterworth filters.\n",
    "butt_filter1,butt_w,butt_h = makefilter.makefiltersos(2000,50,60)\n",
    "window_len = 2.0\n",
    "overlap = 0.5\n",
    "overlap_len = overlap*window_len\n",
    "butt_filter2,butt_w,butt_h = makefilter.makefiltersos(2000,1.0/window_len,0.5/window_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2022100402', '20220713', '20220721', '20221003', '20220810', '20220808', '20220815', '20220816', '20220804', '20220811', '20221005', '2022100401']\n"
     ]
    }
   ],
   "source": [
    "# print(folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = ['20220713','20220721',\n",
    "           '20220804','20220808',\n",
    "           '20220810','20220811',\n",
    "           '20220815','20220816',\n",
    "           '20221003','2022100401',\n",
    "           '2022100402','20221005']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20220713', '20220721', '20220804', '20220808', '20220810', '20220811', '20220815', '20220816', '20221003', '2022100401', '2022100402', '20221005']\n"
     ]
    }
   ],
   "source": [
    "print(folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder=folders[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20220713\n"
     ]
    }
   ],
   "source": [
    "print(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_dic = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = '/ssd/zhibin/1overf/Cleaned_sourcedata/cortical_source_data/' + folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/ssd/zhibin/1overf/Cleaned_sourcedata/cortical_source_data/20220713'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = list_files_in_directory(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['subj1_tr_8.mat',\n",
       " 'subj1_tr_4.mat',\n",
       " 'subj2_tr_4.mat',\n",
       " 'subj2_tr_9.mat',\n",
       " 'subj1_tr_11.mat',\n",
       " 'subj1_tr_5.mat',\n",
       " 'subj1_tr_1.mat',\n",
       " 'subj1_tr_10.mat',\n",
       " 'subj1_tr_3.mat',\n",
       " 'subj2_tr_2.mat',\n",
       " 'subj2_tr_1.mat',\n",
       " 'subj1_tr_6.mat',\n",
       " 'subj1_tr_9.mat',\n",
       " 'subj2_tr_6.mat',\n",
       " 'subj2_tr_3.mat',\n",
       " 'subj2_tr_12.mat',\n",
       " 'subj1_tr_7.mat',\n",
       " 'subj2_tr_8.mat',\n",
       " 'subj2_tr_11.mat',\n",
       " 'subj1_tr_2.mat',\n",
       " 'subj2_tr_7.mat',\n",
       " 'subj2_tr_5.mat',\n",
       " 'subj1_tr_12.mat',\n",
       " 'subj2_tr_10.mat']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in files:\n",
    "    subject, rest = filename.split('_', 1)\n",
    "    if subject not in files_dic:\n",
    "        files_dic[subject] = {}\n",
    "    file = folder + '/' + filename\n",
    "    files_dic[subject][filename] = directory_path + '/' + filename\n",
    "print('files listed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['subj1_tr_8.mat', 'subj1_tr_4.mat', 'subj1_tr_11.mat', 'subj1_tr_5.mat', 'subj1_tr_1.mat', 'subj1_tr_10.mat', 'subj1_tr_3.mat', 'subj1_tr_6.mat', 'subj1_tr_9.mat', 'subj1_tr_7.mat', 'subj1_tr_2.mat', 'subj1_tr_12.mat'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_dic['subj1'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['subj2_tr_4.mat', 'subj2_tr_9.mat', 'subj2_tr_2.mat', 'subj2_tr_1.mat', 'subj2_tr_6.mat', 'subj2_tr_3.mat', 'subj2_tr_12.mat', 'subj2_tr_8.mat', 'subj2_tr_11.mat', 'subj2_tr_7.mat', 'subj2_tr_5.mat', 'subj2_tr_10.mat'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_dic['subj2'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_matrices = {}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "subject 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject='subj1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subj1\n"
     ]
    }
   ],
   "source": [
    "print(subject) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrices = {}\n",
    "subject_files = list(files_dic[subject].keys())\n",
    "subject_files = sorted(subject_files,key=sort_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['subj1_tr_1.mat',\n",
       " 'subj1_tr_2.mat',\n",
       " 'subj1_tr_3.mat',\n",
       " 'subj1_tr_4.mat',\n",
       " 'subj1_tr_5.mat',\n",
       " 'subj1_tr_6.mat',\n",
       " 'subj1_tr_7.mat',\n",
       " 'subj1_tr_8.mat',\n",
       " 'subj1_tr_9.mat',\n",
       " 'subj1_tr_10.mat',\n",
       " 'subj1_tr_11.mat',\n",
       " 'subj1_tr_12.mat']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in subject_files:\n",
    "    print(file)\n",
    "    path = files_dic[subject][file]\n",
    "    filename = folder + '/' + file\n",
    "    # print(path)\n",
    "    #Import the data from a sample file.\n",
    "    data = open_mat_file(path)\n",
    "    #Get signal, filter and downsample.\n",
    "    signal = data['agr_source_data']\n",
    "    filtered_signal = sosfiltfilt(butt_filter1, signal, axis=0)\n",
    "    filtered_signal = sosfiltfilt(butt_filter2, filtered_signal, axis=0)\n",
    "    downsampled_signal = filtered_signal[::10,:]\n",
    "    analytic_signal = hilbert(downsampled_signal,axis=0)\n",
    "    sg = int(np.floor(100/(25))*2+1)\n",
    "    ts1 = savgol_filter(np.real(analytic_signal),sg,1,axis = 0,mode = 'interp')\n",
    "    ts2 = savgol_filter(np.imag(analytic_signal),sg,1,axis = 0,mode = 'interp')\n",
    "    ts3 = ts1+1j*ts2\n",
    "    time_windows = segment_time_series(ts3, window_len, overlap_len, 200)\n",
    "    corr_matrix = []\n",
    "    for window in time_windows:\n",
    "        corr_matrix.append(np.corrcoef(window, rowvar=False))\n",
    "    corr_matrix = np.array(corr_matrix)\n",
    "    corr_matrix = corr_matrix - np.mean(corr_matrix,axis=0)\n",
    "    corr_matrices[filename] = corr_matrix\n",
    "print('correlation matrices calculated.')\n",
    "# 4 min for 12 files in one subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix,sizes = corr_matrix_stack(corr_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['20220713/subj1_tr_1.mat', '20220713/subj1_tr_2.mat', '20220713/subj1_tr_3.mat', '20220713/subj1_tr_4.mat', '20220713/subj1_tr_5.mat', '20220713/subj1_tr_6.mat', '20220713/subj1_tr_7.mat', '20220713/subj1_tr_8.mat', '20220713/subj1_tr_9.mat', '20220713/subj1_tr_10.mat', '20220713/subj1_tr_11.mat', '20220713/subj1_tr_12.mat'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_matrices.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('20220713/subj1_tr_1.mat', 128),\n",
       " ('20220713/subj1_tr_2.mat', 148),\n",
       " ('20220713/subj1_tr_3.mat', 124),\n",
       " ('20220713/subj1_tr_4.mat', 116),\n",
       " ('20220713/subj1_tr_5.mat', 145),\n",
       " ('20220713/subj1_tr_6.mat', 146),\n",
       " ('20220713/subj1_tr_7.mat', 114),\n",
       " ('20220713/subj1_tr_8.mat', 141),\n",
       " ('20220713/subj1_tr_9.mat', 132),\n",
       " ('20220713/subj1_tr_10.mat', 140),\n",
       " ('20220713/subj1_tr_11.mat', 145),\n",
       " ('20220713/subj1_tr_12.mat', 147)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1626, 448, 448)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1626"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_matrix = np.zeros((len(matrix),len(matrix)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in combinations(range(len(matrix)),2):\n",
    "    print(i)\n",
    "    matrix1 = matrix[i,:,:]\n",
    "    matrix2 = matrix[j,:,:]\n",
    "    dist_matrix[i,j] = corr_dist(matrix1,matrix2)\n",
    "    dist_matrix[j,i] = dist_matrix[i,j]\n",
    "        \n",
    "print('distance matrices calculated.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_matrices[subject] = {'distances': dist_matrix, 'sizes':sizes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_matrices.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_matrices['subj1'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(subject_matrices['subj1']['distances'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_matrices['subj1']['sizes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_save = '/ssd/zhibin/1overf/Cleaned_sourcedata/cortical_source_data/Italo/correlation_distances/dyad_' + folder + '_distances.pkl'\n",
    "print(filename_save)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_pickle(subject_matrices, filename_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = ['20220816','2022100401']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = ['2022100401']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022100401\n",
      "files listed.\n",
      "subj1\n",
      "subj1_tr_1.mat\n",
      "subj1_tr_2.mat\n",
      "subj1_tr_3.mat\n",
      "subj1_tr_4.mat\n",
      "subj1_tr_5.mat\n",
      "subj1_tr_6.mat\n",
      "subj1_tr_7.mat\n",
      "subj1_tr_8.mat\n",
      "subj1_tr_9.mat\n",
      "subj1_tr_10.mat\n",
      "subj1_tr_11.mat\n",
      "subj1_tr_12.mat\n",
      "subj2\n",
      "subj2_tr_1.mat\n",
      "subj2_tr_2.mat\n",
      "subj2_tr_3.mat\n",
      "subj2_tr_4.mat\n",
      "subj2_tr_5.mat\n",
      "subj2_tr_6.mat\n",
      "subj2_tr_7.mat\n",
      "subj2_tr_8.mat\n",
      "subj2_tr_9.mat\n",
      "subj2_tr_10.mat\n",
      "subj2_tr_11.mat\n",
      "subj2_tr_12.mat\n"
     ]
    }
   ],
   "source": [
    "for folder in folders:\n",
    "    print(folder)\n",
    "    files_dic = {}\n",
    "#    for folder in folders:\n",
    "#    folder_dic = {}\n",
    "    directory_path = '/ssd/zhibin/1overf/Cleaned_sourcedata/cortical_source_data/' + folder\n",
    "    files = list_files_in_directory(directory_path)\n",
    "\n",
    "    for filename in files:\n",
    "        subject, rest = filename.split('_', 1)\n",
    "        if subject not in files_dic:\n",
    "            files_dic[subject] = {}\n",
    "        file = folder + '/' + filename\n",
    "        files_dic[subject][filename] = directory_path + '/' + filename\n",
    "    print('files listed.')\n",
    "\n",
    "    subject_matrices = {}\n",
    "    for subject in ['subj1','subj2']:\n",
    "        print(subject) \n",
    "        corr_matrices = {}\n",
    "        subject_files = list(files_dic[subject].keys())\n",
    "        subject_files = sorted(subject_files,key=sort_key)\n",
    "\n",
    "        for file in subject_files:\n",
    "            print(file)\n",
    "            path = files_dic[subject][file]\n",
    "            filename = folder + '/' + file\n",
    "            # print(path)\n",
    "\n",
    "            #Import the data from a sample file.\n",
    "            data = open_mat_file(path)\n",
    "\n",
    "            #Get signal, filter and downsample.\n",
    "            signal = data['agr_source_data']\n",
    "            filtered_signal = sosfiltfilt(butt_filter1, signal, axis=0)\n",
    "\n",
    "            filtered_signal = sosfiltfilt(butt_filter2, filtered_signal, axis=0)\n",
    "            downsampled_signal = filtered_signal[::10,:]\n",
    "\n",
    "            analytic_signal = hilbert(downsampled_signal,axis=0)\n",
    "            sg = int(np.floor(100/(25))*2+1)\n",
    "            ts1 = savgol_filter(np.real(analytic_signal),sg,1,axis = 0,mode = 'interp')\n",
    "            ts2 = savgol_filter(np.imag(analytic_signal),sg,1,axis = 0,mode = 'interp')\n",
    "            ts3 = ts1+1j*ts2\n",
    "            time_windows = segment_time_series(ts3, window_len, overlap_len, 200)\n",
    "\n",
    "            corr_matrix = []\n",
    "            for window in time_windows:\n",
    "                corr_matrix.append(np.corrcoef(window, rowvar=False))\n",
    "            corr_matrix = np.array(corr_matrix)\n",
    "            corr_matrix = corr_matrix - np.mean(corr_matrix,axis=0)\n",
    "            corr_matrices[filename] = corr_matrix\n",
    "        print('correlation matrices calculated.')\n",
    "\n",
    "        matrix,sizes = corr_matrix_stack(corr_matrices)\n",
    "        dist_matrix = np.zeros((len(matrix),len(matrix)))\n",
    "        for i,j in combinations(range(len(matrix)),2):\n",
    "            print(i)\n",
    "            matrix1 = matrix[i,:,:]\n",
    "            matrix2 = matrix[j,:,:]\n",
    "            dist_matrix[i,j] = corr_dist(matrix1,matrix2)\n",
    "            dist_matrix[j,i] = dist_matrix[i,j]\n",
    "        \n",
    "        print('distance matrices calculated.')\n",
    "        subject_matrices[subject] = {'distances': dist_matrix, 'sizes':sizes}\n",
    "    \n",
    "    filename_save = '/ssd/zhibin/1overf/Cleaned_sourcedata/cortical_source_data/Italo/correlation_distances/dyad_' + folder + '_distances.pkl'\n",
    "    print(filename_save)\n",
    "    save_to_pickle(subject_matrices, filename_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/ssd/zhibin/1overf/Cleaned_sourcedata/cortical_source_data/2022100401/subj1_tr_6.mat'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 5] Input/output error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2120357/1059274162.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen_mat_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_2120357/3354858818.py\u001b[0m in \u001b[0;36mopen_mat_file\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \"\"\"\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m# Load the .mat file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/eeg/lib/python3.9/site-packages/scipy/io/matlab/_mio.py\u001b[0m in \u001b[0;36mloadmat\u001b[0;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0mMR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmat_reader_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m         \u001b[0mmatfile_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmdict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/eeg/lib/python3.9/site-packages/scipy/io/matlab/_mio5.py\u001b[0m in \u001b[0;36mget_variables\u001b[0;34m(self, variable_names)\u001b[0m\n\u001b[1;32m    330\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_var_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhdr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mMatReadError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m                 warnings.warn(\n",
      "\u001b[0;32m~/anaconda3/envs/eeg/lib/python3.9/site-packages/scipy/io/matlab/_mio5.py\u001b[0m in \u001b[0;36mread_var_array\u001b[0;34m(self, header, process)\u001b[0m\n\u001b[1;32m    290\u001b[0m            \u001b[0;31m`\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         '''\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_matrix_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_from_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariable_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m_mio5_utils.pyx\u001b[0m in \u001b[0;36mscipy.io.matlab._mio5_utils.VarReader5.array_from_header\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_mio5_utils.pyx\u001b[0m in \u001b[0;36mscipy.io.matlab._mio5_utils.VarReader5.array_from_header\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_mio5_utils.pyx\u001b[0m in \u001b[0;36mscipy.io.matlab._mio5_utils.VarReader5.read_real_complex\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_mio5_utils.pyx\u001b[0m in \u001b[0;36mscipy.io.matlab._mio5_utils.VarReader5.read_numeric\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_mio5_utils.pyx\u001b[0m in \u001b[0;36mscipy.io.matlab._mio5_utils.VarReader5.read_element\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_streams.pyx\u001b[0m in \u001b[0;36mscipy.io.matlab._streams.ZlibInputStream.read_string\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_streams.pyx\u001b[0m in \u001b[0;36mscipy.io.matlab._streams.ZlibInputStream.read_into\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_streams.pyx\u001b[0m in \u001b[0;36mscipy.io.matlab._streams.ZlibInputStream._fill_buffer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 5] Input/output error"
     ]
    }
   ],
   "source": [
    "data = open_mat_file(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eeg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
