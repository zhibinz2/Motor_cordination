{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import makefilter\n",
    "from scipy.signal import sosfiltfilt, hilbert\n",
    "from scipy.signal import savgol_filter\n",
    "import pickle\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def list_data_folders(directory_path,pattern):\n",
    "    \"\"\"\n",
    "    Lists all folders in the given directory that start with pattern.\n",
    "    \n",
    "    Parameters:\n",
    "    - directory_path: A string representing the path to the directory to search in.\n",
    "\n",
    "    - pattern: A string representing the pattern at the beginning of the folders' name.\n",
    "    \n",
    "    Returns:\n",
    "    - A list of folder names that meet the criteria.\n",
    "    \"\"\"\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(directory_path):\n",
    "        print(f\"The directory {directory_path} does not exist.\")\n",
    "        return []\n",
    "\n",
    "    # Get all items in the directory\n",
    "    all_items = os.listdir(directory_path)\n",
    "\n",
    "    # Filter for directories that start with pattern\n",
    "    folders_starting_with_pattern = [item for item in all_items\n",
    "                                if os.path.isdir(os.path.join(directory_path, item)) and item.startswith(pattern)]\n",
    "\n",
    "    return folders_starting_with_pattern\n",
    "\n",
    "def list_files_in_directory(directory_path):\n",
    "    \"\"\"\n",
    "    Lists all the files in the specified directory.\n",
    "    \n",
    "    Parameters:\n",
    "    - directory_path: A string representing the path to the directory to search in.\n",
    "    \n",
    "    Returns:\n",
    "    - A list of file names contained in the directory.\n",
    "    \"\"\"\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(directory_path):\n",
    "        print(f\"The directory {directory_path} does not exist.\")\n",
    "        return []\n",
    "\n",
    "    # Get all items in the directory\n",
    "    all_items = os.listdir(directory_path)\n",
    "\n",
    "    # Filter out only files\n",
    "    files_only = [item for item in all_items if os.path.isfile(os.path.join(directory_path, item))]\n",
    "\n",
    "    return files_only\n",
    "\n",
    "def open_mat_file(file_path):\n",
    "    \"\"\"\n",
    "    Opens a .mat file and returns its contents.\n",
    "    \n",
    "    Parameters:\n",
    "    - file_path: A string representing the path to the .mat file.\n",
    "    \n",
    "    Returns:\n",
    "    - A dictionary containing variables loaded from the .mat file.\n",
    "    \"\"\"\n",
    "    # Load the .mat file\n",
    "    data = loadmat(file_path)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def segment_time_series(data, segment_duration, overlap_duration, sampling_rate):\n",
    "    \"\"\"\n",
    "    Segments each time series in a 2D NumPy array into smaller time series segments \n",
    "    based on specified duration, overlap, and sampling rate.\n",
    "\n",
    "    Parameters:\n",
    "    - data (numpy.ndarray): A 2D NumPy array containing the time series data. Each \n",
    "    column represents a single time series.\n",
    "    - segment_duration (float): The duration of each segment in seconds. Determines\n",
    "     the length of each segment generated.\n",
    "    - overlap_duration (float): The duration of overlap between consecutive segments\n",
    "     in seconds. This specifies how much each segment should overlap with the next.\n",
    "    - sampling_rate (int): The number of samples per second in the time series data.\n",
    "     This is used to calculate the number of samples per segment and the overlap in \n",
    "     samples.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: A 3D NumPy array where the 1st dimension represents a segmented portion \n",
    "    of the original time series. The 2nd dimension in this array equals the number \n",
    "    of samples per segment, determined by the `segment_duration` and `sampling_rate`.\n",
    "    \"\"\"\n",
    "    # Calculate parameters\n",
    "    samples_per_segment = int(segment_duration * sampling_rate)\n",
    "    overlap_samples = int(overlap_duration * sampling_rate)\n",
    "    step_size = samples_per_segment - overlap_samples\n",
    "    \n",
    "    segments = []\n",
    "    len_data,_ = data.shape\n",
    "    for start in range(0, len_data - samples_per_segment + 1, step_size):\n",
    "        segment = data[start:start+samples_per_segment,:]\n",
    "        segments.append(segment)\n",
    "            \n",
    "    return np.array(segments)\n",
    "\n",
    "def save_to_pickle(dict_obj, file_path):\n",
    "    \"\"\"\n",
    "    Saves a given dictionary to a pickle file.\n",
    "\n",
    "    Parameters:\n",
    "    - dict_obj (dict): The dictionary to be saved.\n",
    "    - file_path (str): The path to the file where the dictionary will be saved.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    with open(file_path, 'wb') as file:\n",
    "        # Serialize the dictionary and save it to the file\n",
    "        pickle.dump(dict_obj, file)\n",
    "    print(f\"Data successfully saved to {file_path}.\")\n",
    "\n",
    "def corr_matrix_stack(corr_matrix_dic):\n",
    "    \"\"\"\n",
    "    Stacks correlation matrices along axis 0 and tracks their sizes and identifiers.\n",
    "\n",
    "    This function takes a dictionary of correlation matrices (2D NumPy arrays) and\n",
    "    stacks these matrices vertically. It also compiles a list of the sizes of these\n",
    "    matrices along axis 0, alongside their identifiers.\n",
    "\n",
    "    Parameters:\n",
    "    - corr_matrix_dic (dict): A dictionary where the keys are identifiers (e.g., file names)\n",
    "      and the values are correlation matrices (2D NumPy arrays). Each matrix is assumed to\n",
    "      have the same number of columns but can vary in the number of rows.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: A single 2D NumPy array resulting from stacking all the input matrices\n",
    "      along axis 0.\n",
    "    - list of tuples: Each tuple contains an identifier (key from the input dictionary) and\n",
    "      an integer representing the size (number of rows) of the corresponding matrix before\n",
    "      stacking. This list maintains the order in which matrices were stacked.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize a list to keep track of each matrix's identifier and its number of rows\n",
    "    size_matrices = []\n",
    "    \n",
    "    # Initialize a list to hold all matrices for concatenation\n",
    "    matrix_list = []\n",
    "    \n",
    "    # Iterate over the dictionary items\n",
    "    for file, matrix in corr_matrix_dic.items():\n",
    "        # Append each matrix to the list for later concatenation\n",
    "        matrix_list.append(matrix)\n",
    "        # Append a tuple of the matrix's identifier and its number of rows to the tracking list\n",
    "        size_matrices.append((file, matrix.shape[0]))\n",
    "    \n",
    "    # Concatenate all matrices vertically\n",
    "    stacked_array = np.concatenate(matrix_list, axis=0)\n",
    "    \n",
    "    # Return the stacked array and the list of identifiers with their corresponding matrix sizes\n",
    "    return stacked_array, size_matrices\n",
    "\n",
    "def corr_dist(A,B):\n",
    "    from scipy.linalg import norm\n",
    "    D = np.transpose(np.conj(A))@B\n",
    "    dist = np.real(np.log(1.0/(np.trace(D)/(norm(A)*norm(B)))))\n",
    "    return dist\n",
    "\n",
    "def sort_key(filename):\n",
    "    trial_part = int(filename.split('_')[-1].split('.')[0])  # Extract the trial number\n",
    "    return trial_part\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_time_series_taplocked(data, segment_duration, sampling_rate, tr_samples):\n",
    "    \"\"\"\n",
    "    Segments each time series in a 2D NumPy array into smaller time series segments \n",
    "    based on specified duration, and sampling rate.\n",
    "\n",
    "    Parameters:\n",
    "    - data (numpy.ndarray): A 2D NumPy array containing the time series data. Each \n",
    "    column represents a single time series.\n",
    "    - segment_duration (float): The duration of each segment in seconds. Determines\n",
    "     the length of each segment generated.\n",
    "    - sampling_rate (int): The number of samples per second in the time series data.\n",
    "     This is used to calculate the number of samples per segment and the overlap in \n",
    "     samples.\n",
    "    - tr_samples: index of taps in the samples of a single trial\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: A 3D NumPy array where the 1st dimension represents a segmented portion \n",
    "    of the original time series. The 2nd dimension in this array equals the number \n",
    "    of samples per segment, determined by the `segment_duration` and `sampling_rate`.\n",
    "    \"\"\"\n",
    "    # Calculate parameters\n",
    "    samples_per_segment = int(segment_duration * sampling_rate)\n",
    "\n",
    "    segments = []\n",
    "    for start in tr_samples:\n",
    "        segment = data[start-samples_per_segment:start,:]\n",
    "        segments.append(segment)\n",
    "\n",
    "    return np.array(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory_path = '/ssd/zhibin/1overf/Cleaned_sourcedata/cortical_source_data/'  # Replace this with the path to your directory\n",
    "# folders = list_data_folders(directory_path,'20')\n",
    "\n",
    "#Butterworth filters.\n",
    "butt_filter1,butt_w,butt_h = makefilter.makefiltersos(2000,50,60)\n",
    "# window_len = 2.0\n",
    "window_len = 0.5\n",
    "# overlap = 0.5\n",
    "# overlap_len = overlap*window_len\n",
    "butt_filter2,butt_w,butt_h = makefilter.makefiltersos(2000,1.0/window_len,0.5/window_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = ['20220713','20220721',\n",
    "           '20220804','20220808',\n",
    "           '20220810','20220811',\n",
    "           '20220815','20220816',\n",
    "           '20221003','2022100401',\n",
    "           '2022100402','20221005']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder=folders[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20220713\n"
     ]
    }
   ],
   "source": [
    "print(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_dic = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = '/ssd/zhibin/1overf/Cleaned_sourcedata/cortical_source_data/' + folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/ssd/zhibin/1overf/Cleaned_sourcedata/cortical_source_data/20220713'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = list_files_in_directory(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['subj1_tr_8.mat',\n",
       " 'subj1_tr_4.mat',\n",
       " 'subj2_tr_4.mat',\n",
       " 'subj2_tr_9.mat',\n",
       " 'subj1_tr_11.mat',\n",
       " 'subj1_tr_5.mat',\n",
       " 'subj1_tr_1.mat',\n",
       " 'subj1_tr_10.mat',\n",
       " 'subj1_tr_3.mat',\n",
       " 'subj2_tr_2.mat',\n",
       " 'subj2_tr_1.mat',\n",
       " 'subj1_tr_6.mat',\n",
       " 'subj1_tr_9.mat',\n",
       " 'subj2_tr_6.mat',\n",
       " 'subj2_tr_3.mat',\n",
       " 'subj2_tr_12.mat',\n",
       " 'subj1_tr_7.mat',\n",
       " 'subj2_tr_8.mat',\n",
       " 'subj2_tr_11.mat',\n",
       " 'subj1_tr_2.mat',\n",
       " 'subj2_tr_7.mat',\n",
       " 'subj2_tr_5.mat',\n",
       " 'subj1_tr_12.mat',\n",
       " 'subj2_tr_10.mat']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load event time points\n",
    "beha_filename = '/home/zhibinz2/Documents/GitHub/finger_tapping_behavioral_data/clean_' + str(folder) + '_bpchan.mat'\n",
    "beh_data =  loadmat(beha_filename)\n",
    "samples = beh_data['samples'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute difference(error) of the samples\n",
    "tr=0\n",
    "tr_samples = samples[tr]\n",
    "num_tp=len(tr_samples)\n",
    "tr_error=np.zeros(num_tp)\n",
    "for tp in range(num_tp):\n",
    "    tr_error[tp]=np.abs(tr_samples[tp][0]-tr_samples[tp][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files listed.\n"
     ]
    }
   ],
   "source": [
    "for filename in files:\n",
    "    subject, rest = filename.split('_', 1)\n",
    "    if subject not in files_dic:\n",
    "        files_dic[subject] = {}\n",
    "    file = folder + '/' + filename\n",
    "    files_dic[subject][filename] = directory_path + '/' + filename\n",
    "print('files listed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['subj1_tr_8.mat', 'subj1_tr_4.mat', 'subj1_tr_11.mat', 'subj1_tr_5.mat', 'subj1_tr_1.mat', 'subj1_tr_10.mat', 'subj1_tr_3.mat', 'subj1_tr_6.mat', 'subj1_tr_9.mat', 'subj1_tr_7.mat', 'subj1_tr_2.mat', 'subj1_tr_12.mat'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_dic['subj1'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['subj2_tr_4.mat', 'subj2_tr_9.mat', 'subj2_tr_2.mat', 'subj2_tr_1.mat', 'subj2_tr_6.mat', 'subj2_tr_3.mat', 'subj2_tr_12.mat', 'subj2_tr_8.mat', 'subj2_tr_11.mat', 'subj2_tr_7.mat', 'subj2_tr_5.mat', 'subj2_tr_10.mat'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_dic['subj2'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_matrices = {}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "subject 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject='subj1'\n",
    "subj=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subj1\n"
     ]
    }
   ],
   "source": [
    "print(subject) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrices = {}\n",
    "subject_files = list(files_dic[subject].keys())\n",
    "subject_files = sorted(subject_files,key=sort_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['subj1_tr_1.mat',\n",
       " 'subj1_tr_2.mat',\n",
       " 'subj1_tr_3.mat',\n",
       " 'subj1_tr_4.mat',\n",
       " 'subj1_tr_5.mat',\n",
       " 'subj1_tr_6.mat',\n",
       " 'subj1_tr_7.mat',\n",
       " 'subj1_tr_8.mat',\n",
       " 'subj1_tr_9.mat',\n",
       " 'subj1_tr_10.mat',\n",
       " 'subj1_tr_11.mat',\n",
       " 'subj1_tr_12.mat']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20220713/subj1_tr_1.mat'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr=1\n",
    "file='subj1_tr_1.mat'\n",
    "path = files_dic[subject][file]\n",
    "filename = folder + '/' + file\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/ssd/zhibin/1overf/Cleaned_sourcedata/cortical_source_data/20220713/subj1_tr_1.mat'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the data from a sample file.\n",
    "data = open_mat_file(path)\n",
    "#Get signal, filter and downsample.\n",
    "signal = data['agr_source_data']\n",
    "filtered_signal = sosfiltfilt(butt_filter1, signal, axis=0)\n",
    "filtered_signal = sosfiltfilt(butt_filter2, filtered_signal, axis=0)\n",
    "# downsampled_signal = filtered_signal[::10,:]\n",
    "# analytic_signal = hilbert(downsampled_signal,axis=0)\n",
    "# no downsampling\n",
    "analytic_signal = hilbert(filtered_signal,axis=0)\n",
    "sg = int(np.floor(100/(25))*2+1)\n",
    "ts1 = savgol_filter(np.real(analytic_signal),sg,1,axis = 0,mode = 'interp')\n",
    "ts2 = savgol_filter(np.imag(analytic_signal),sg,1,axis = 0,mode = 'interp')\n",
    "ts3 = ts1+1j*ts2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_windows = segment_time_series(ts3, window_len, overlap_len, 200)\n",
    "# segment data with tap-locked windows\n",
    "tr_samples_subj = samples[tr][:,subj]\n",
    "time_windows = segment_time_series_taplocked(ts3, window_len, 2000, tr_samples_subj)\n",
    "# 50s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 448)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(time_windows[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = []\n",
    "for window in time_windows:\n",
    "    corr_matrix.append(np.corrcoef(window, rowvar=False))\n",
    "corr_matrix = np.array(corr_matrix)\n",
    "corr_matrix = corr_matrix - np.mean(corr_matrix,axis=0)\n",
    "corr_matrices[filename] = corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "subj1_tr_1.mat\n",
      "1\n",
      "subj1_tr_2.mat\n",
      "2\n",
      "subj1_tr_3.mat\n",
      "3\n",
      "subj1_tr_4.mat\n",
      "4\n",
      "subj1_tr_5.mat\n",
      "5\n",
      "subj1_tr_6.mat\n",
      "6\n",
      "subj1_tr_7.mat\n",
      "7\n",
      "subj1_tr_8.mat\n",
      "8\n",
      "subj1_tr_9.mat\n",
      "9\n",
      "subj1_tr_10.mat\n",
      "10\n",
      "subj1_tr_11.mat\n",
      "11\n",
      "subj1_tr_12.mat\n",
      "correlation matrices calculated.\n"
     ]
    }
   ],
   "source": [
    "for tr, file in enumerate(subject_files):\n",
    "    print(tr)\n",
    "    print(file)\n",
    "    path = files_dic[subject][file]\n",
    "    filename = folder + '/' + file\n",
    "    # print(path)\n",
    "    #Import the data from a sample file.\n",
    "    data = open_mat_file(path)\n",
    "    #Get signal, filter and downsample.\n",
    "    signal = data['agr_source_data']\n",
    "    filtered_signal = sosfiltfilt(butt_filter1, signal, axis=0)\n",
    "    filtered_signal = sosfiltfilt(butt_filter2, filtered_signal, axis=0)\n",
    "    # downsampled_signal = filtered_signal[::10,:]\n",
    "    # analytic_signal = hilbert(downsampled_signal,axis=0)\n",
    "    # no downsampling\n",
    "    analytic_signal = hilbert(filtered_signal,axis=0)\n",
    "    sg = int(np.floor(100/(25))*2+1)\n",
    "    ts1 = savgol_filter(np.real(analytic_signal),sg,1,axis = 0,mode = 'interp')\n",
    "    ts2 = savgol_filter(np.imag(analytic_signal),sg,1,axis = 0,mode = 'interp')\n",
    "    ts3 = ts1+1j*ts2\n",
    "    \n",
    "    # time_windows = segment_time_series(ts3, window_len, overlap_len, 200)\n",
    "    # segment data with tap-locked windows\n",
    "    tr_samples = samples[tr][:,subj]\n",
    "    time_windows = segment_time_series_taplocked(ts3, window_len, 2000, tr_samples)\n",
    "\n",
    "    corr_matrix = []\n",
    "    for window in time_windows:\n",
    "        corr_matrix.append(np.corrcoef(window, rowvar=False))\n",
    "    corr_matrix = np.array(corr_matrix)\n",
    "    corr_matrix = corr_matrix - np.mean(corr_matrix,axis=0)\n",
    "    corr_matrices[filename] = corr_matrix\n",
    "print('correlation matrices calculated.')\n",
    "# 9 min for 12 files in one subject\n",
    "# 9 x 12 files x 2 subjects x 12 session = 43 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix,sizes = corr_matrix_stack(corr_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['20220713/subj1_tr_1.mat', '20220713/subj1_tr_2.mat', '20220713/subj1_tr_3.mat', '20220713/subj1_tr_4.mat', '20220713/subj1_tr_5.mat', '20220713/subj1_tr_6.mat', '20220713/subj1_tr_7.mat', '20220713/subj1_tr_8.mat', '20220713/subj1_tr_9.mat', '20220713/subj1_tr_10.mat', '20220713/subj1_tr_11.mat', '20220713/subj1_tr_12.mat'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_matrices.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('20220713/subj1_tr_1.mat', 128),\n",
       " ('20220713/subj1_tr_2.mat', 148),\n",
       " ('20220713/subj1_tr_3.mat', 124),\n",
       " ('20220713/subj1_tr_4.mat', 116),\n",
       " ('20220713/subj1_tr_5.mat', 145),\n",
       " ('20220713/subj1_tr_6.mat', 146),\n",
       " ('20220713/subj1_tr_7.mat', 114),\n",
       " ('20220713/subj1_tr_8.mat', 141),\n",
       " ('20220713/subj1_tr_9.mat', 132),\n",
       " ('20220713/subj1_tr_10.mat', 140),\n",
       " ('20220713/subj1_tr_11.mat', 145),\n",
       " ('20220713/subj1_tr_12.mat', 147)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1626, 448, 448)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1626"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_matrix = np.zeros((len(matrix),len(matrix)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in combinations(range(len(matrix)),2):\n",
    "    print(i)\n",
    "    matrix1 = matrix[i,:,:]\n",
    "    matrix2 = matrix[j,:,:]\n",
    "    dist_matrix[i,j] = corr_dist(matrix1,matrix2)\n",
    "    dist_matrix[j,i] = dist_matrix[i,j]\n",
    "        \n",
    "print('distance matrices calculated.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_matrices[subject] = {'distances': dist_matrix, 'sizes':sizes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_matrices.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_matrices['subj1'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(subject_matrices['subj1']['distances'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_matrices['subj1']['sizes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022100401\n",
      "files listed.\n",
      "subj1\n",
      "subj1_tr_1.mat\n",
      "subj1_tr_2.mat\n",
      "subj1_tr_3.mat\n",
      "subj1_tr_4.mat\n",
      "subj1_tr_5.mat\n",
      "subj1_tr_6.mat\n",
      "subj1_tr_7.mat\n",
      "subj1_tr_8.mat\n",
      "subj1_tr_9.mat\n",
      "subj1_tr_10.mat\n",
      "subj1_tr_11.mat\n",
      "subj1_tr_12.mat\n",
      "subj2\n",
      "subj2_tr_1.mat\n",
      "subj2_tr_2.mat\n",
      "subj2_tr_3.mat\n",
      "subj2_tr_4.mat\n",
      "subj2_tr_5.mat\n",
      "subj2_tr_6.mat\n",
      "subj2_tr_7.mat\n",
      "subj2_tr_8.mat\n",
      "subj2_tr_9.mat\n",
      "subj2_tr_10.mat\n",
      "subj2_tr_11.mat\n",
      "subj2_tr_12.mat\n"
     ]
    }
   ],
   "source": [
    "for folder in folders:\n",
    "    print(folder)\n",
    "    files_dic = {}\n",
    "#    for folder in folders:\n",
    "#    folder_dic = {}\n",
    "    directory_path = '/ssd/zhibin/1overf/Cleaned_sourcedata/cortical_source_data/' + folder\n",
    "    files = list_files_in_directory(directory_path)\n",
    "\n",
    "    # load event time points\n",
    "    beha_filename = '/home/zhibinz2/Documents/GitHub/finger_tapping_behavioral_data/clean_' + str(folder) + '_bpchan.mat'\n",
    "    beh_data =  loadmat(beha_filename)\n",
    "    samples = beh_data['samples'][0]\n",
    "\n",
    "    for filename in files:\n",
    "        subject, rest = filename.split('_', 1)\n",
    "        if subject not in files_dic:\n",
    "            files_dic[subject] = {}\n",
    "        file = folder + '/' + filename\n",
    "        files_dic[subject][filename] = directory_path + '/' + filename\n",
    "    print('files listed.')\n",
    "\n",
    "    subject_matrices = {}\n",
    "    for subj,subject in enumerate(['subj1','subj2']):\n",
    "        print(subj)\n",
    "        print(subject) \n",
    "        corr_matrices = {}\n",
    "        subject_files = list(files_dic[subject].keys())\n",
    "        subject_files = sorted(subject_files,key=sort_key)\n",
    "        \n",
    "\n",
    "        for tr, file in enumerate(subject_files):\n",
    "            print(tr)\n",
    "            print(file)\n",
    "            path = files_dic[subject][file]\n",
    "            filename = folder + '/' + file\n",
    "            # print(path)\n",
    "\n",
    "            #Import the data from a sample file.\n",
    "            data = open_mat_file(path)\n",
    "\n",
    "            #Get signal, filter and downsample.\n",
    "            signal = data['agr_source_data']\n",
    "            filtered_signal = sosfiltfilt(butt_filter1, signal, axis=0)\n",
    "            filtered_signal = sosfiltfilt(butt_filter2, filtered_signal, axis=0)\n",
    "            # downsampled_signal = filtered_signal[::10,:]\n",
    "            # analytic_signal = hilbert(downsampled_signal,axis=0)\n",
    "            # no downsampling\n",
    "            analytic_signal = hilbert(filtered_signal,axis=0)\n",
    "\n",
    "            sg = int(np.floor(100/(25))*2+1)\n",
    "            ts1 = savgol_filter(np.real(analytic_signal),sg,1,axis = 0,mode = 'interp')\n",
    "            ts2 = savgol_filter(np.imag(analytic_signal),sg,1,axis = 0,mode = 'interp')\n",
    "            ts3 = ts1+1j*ts2\n",
    "            \n",
    "            # time_windows = segment_time_series(ts3, window_len, overlap_len, 200)\n",
    "            # segment data with tap-locked windows\n",
    "            tr_samples = samples[tr][:,subj]\n",
    "            time_windows = segment_time_series_taplocked(ts3, window_len, 2000,tr_samples)\n",
    "            \n",
    "            corr_matrix = []\n",
    "            for window in time_windows:\n",
    "                corr_matrix.append(np.corrcoef(window, rowvar=False))\n",
    "            corr_matrix = np.array(corr_matrix)\n",
    "            corr_matrix = corr_matrix - np.mean(corr_matrix,axis=0)\n",
    "            corr_matrices[filename] = corr_matrix\n",
    "        print('correlation matrices calculated.')\n",
    "\n",
    "        matrix,sizes = corr_matrix_stack(corr_matrices)\n",
    "        dist_matrix = np.zeros((len(matrix),len(matrix)))\n",
    "        for i,j in combinations(range(len(matrix)),2):\n",
    "            print(i)\n",
    "            matrix1 = matrix[i,:,:]\n",
    "            matrix2 = matrix[j,:,:]\n",
    "            dist_matrix[i,j] = corr_dist(matrix1,matrix2)\n",
    "            dist_matrix[j,i] = dist_matrix[i,j]\n",
    "        \n",
    "        print('distance matrices calculated.')\n",
    "        subject_matrices[subject] = {'distances': dist_matrix, 'sizes':sizes}\n",
    "    \n",
    "    # filename_save = '/ssd/zhibin/1overf/Cleaned_sourcedata/cortical_source_data/Italo/correlation_distances/dyad_' + folder + '_distances.pkl'\n",
    "    filename_save = '/ssd/zhibin/1overf/Cleaned_sourcedata/cortical_source_data/taplocked/correlation_distances/dyad_' + folder + '_distances.pkl'\n",
    "    print(filename_save)\n",
    "    save_to_pickle(subject_matrices, filename_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/ssd/zhibin/1overf/Cleaned_sourcedata/cortical_source_data/2022100401/subj1_tr_6.mat'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open_mat_file(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tr12'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20220713\n",
      "files listed.\n",
      "subj1\n",
      "subj1_tr_1.mat\n",
      "subj1_tr_2.mat\n",
      "subj1_tr_3.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2168604/3717702527.py:29: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(segments)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subj1_tr_4.mat\n",
      "subj1_tr_5.mat\n",
      "subj1_tr_6.mat\n",
      "subj1_tr_7.mat\n",
      "subj1_tr_8.mat\n",
      "subj1_tr_9.mat\n",
      "subj1_tr_10.mat\n",
      "subj1_tr_11.mat\n",
      "subj1_tr_12.mat\n",
      "subj2\n",
      "subj2_tr_1.mat\n",
      "subj2_tr_2.mat\n",
      "subj2_tr_3.mat\n",
      "subj2_tr_4.mat\n",
      "subj2_tr_5.mat\n",
      "subj2_tr_6.mat\n",
      "subj2_tr_7.mat\n",
      "subj2_tr_8.mat\n",
      "subj2_tr_9.mat\n",
      "subj2_tr_10.mat\n",
      "subj2_tr_11.mat\n",
      "subj2_tr_12.mat\n",
      "20220721\n",
      "files listed.\n",
      "subj1\n",
      "subj1_tr_1.mat\n",
      "subj1_tr_2.mat\n",
      "subj1_tr_3.mat\n",
      "subj1_tr_4.mat\n",
      "subj1_tr_5.mat\n",
      "subj1_tr_6.mat\n",
      "subj1_tr_7.mat\n",
      "subj1_tr_8.mat\n",
      "subj1_tr_9.mat\n",
      "subj1_tr_10.mat\n",
      "subj1_tr_11.mat\n",
      "subj1_tr_12.mat\n",
      "subj2\n",
      "subj2_tr_1.mat\n",
      "subj2_tr_2.mat\n",
      "subj2_tr_3.mat\n",
      "subj2_tr_4.mat\n",
      "subj2_tr_5.mat\n",
      "subj2_tr_6.mat\n",
      "subj2_tr_7.mat\n",
      "subj2_tr_8.mat\n",
      "subj2_tr_9.mat\n",
      "subj2_tr_10.mat\n",
      "subj2_tr_11.mat\n",
      "subj2_tr_12.mat\n",
      "20220804\n",
      "files listed.\n",
      "subj1\n",
      "subj1_tr_1.mat\n",
      "subj1_tr_2.mat\n",
      "subj1_tr_3.mat\n",
      "subj1_tr_4.mat\n",
      "subj1_tr_5.mat\n",
      "subj1_tr_6.mat\n",
      "subj1_tr_7.mat\n",
      "subj1_tr_8.mat\n",
      "subj1_tr_9.mat\n",
      "subj1_tr_10.mat\n",
      "subj1_tr_11.mat\n",
      "subj1_tr_12.mat\n",
      "subj2\n",
      "subj2_tr_1.mat\n",
      "subj2_tr_2.mat\n",
      "subj2_tr_3.mat\n",
      "subj2_tr_4.mat\n",
      "subj2_tr_5.mat\n",
      "subj2_tr_6.mat\n",
      "subj2_tr_7.mat\n",
      "subj2_tr_8.mat\n",
      "subj2_tr_9.mat\n",
      "subj2_tr_10.mat\n",
      "subj2_tr_11.mat\n",
      "subj2_tr_12.mat\n",
      "20220808\n",
      "files listed.\n",
      "subj1\n",
      "subj1_tr_1.mat\n",
      "subj1_tr_2.mat\n",
      "subj1_tr_3.mat\n",
      "subj1_tr_4.mat\n",
      "subj1_tr_5.mat\n",
      "subj1_tr_6.mat\n",
      "subj1_tr_7.mat\n",
      "subj1_tr_8.mat\n",
      "subj1_tr_9.mat\n",
      "subj1_tr_10.mat\n",
      "subj1_tr_11.mat\n",
      "subj1_tr_12.mat\n",
      "subj2\n",
      "subj2_tr_1.mat\n",
      "subj2_tr_2.mat\n",
      "subj2_tr_3.mat\n",
      "subj2_tr_4.mat\n",
      "subj2_tr_5.mat\n",
      "subj2_tr_6.mat\n",
      "subj2_tr_7.mat\n",
      "subj2_tr_8.mat\n",
      "subj2_tr_9.mat\n",
      "subj2_tr_10.mat\n",
      "subj2_tr_11.mat\n",
      "subj2_tr_12.mat\n",
      "20220810\n",
      "files listed.\n",
      "subj1\n",
      "subj1_tr_1.mat\n",
      "subj1_tr_2.mat\n",
      "subj1_tr_3.mat\n",
      "subj1_tr_4.mat\n",
      "subj1_tr_5.mat\n",
      "subj1_tr_6.mat\n",
      "subj1_tr_7.mat\n",
      "subj1_tr_8.mat\n",
      "subj1_tr_9.mat\n",
      "subj1_tr_10.mat\n",
      "subj1_tr_11.mat\n",
      "subj1_tr_12.mat\n",
      "subj2\n",
      "subj2_tr_1.mat\n",
      "subj2_tr_2.mat\n",
      "subj2_tr_3.mat\n",
      "subj2_tr_4.mat\n",
      "subj2_tr_5.mat\n",
      "subj2_tr_6.mat\n",
      "subj2_tr_7.mat\n",
      "subj2_tr_8.mat\n",
      "subj2_tr_9.mat\n",
      "subj2_tr_10.mat\n",
      "subj2_tr_11.mat\n",
      "subj2_tr_12.mat\n",
      "20220811\n",
      "files listed.\n",
      "subj1\n",
      "subj1_tr_1.mat\n",
      "subj1_tr_2.mat\n",
      "subj1_tr_3.mat\n",
      "subj1_tr_4.mat\n",
      "subj1_tr_5.mat\n",
      "subj1_tr_6.mat\n",
      "subj1_tr_7.mat\n",
      "subj1_tr_8.mat\n",
      "subj1_tr_9.mat\n",
      "subj1_tr_10.mat\n",
      "subj1_tr_11.mat\n",
      "subj1_tr_12.mat\n",
      "subj2\n",
      "subj2_tr_1.mat\n",
      "subj2_tr_2.mat\n",
      "subj2_tr_3.mat\n",
      "subj2_tr_4.mat\n",
      "subj2_tr_5.mat\n",
      "subj2_tr_6.mat\n",
      "subj2_tr_7.mat\n",
      "subj2_tr_8.mat\n",
      "subj2_tr_9.mat\n",
      "subj2_tr_10.mat\n",
      "subj2_tr_11.mat\n",
      "subj2_tr_12.mat\n",
      "20220815\n",
      "files listed.\n",
      "subj1\n",
      "subj1_tr_1.mat\n",
      "subj1_tr_2.mat\n",
      "subj1_tr_3.mat\n",
      "subj1_tr_4.mat\n",
      "subj1_tr_5.mat\n",
      "subj1_tr_6.mat\n",
      "subj1_tr_7.mat\n",
      "subj1_tr_8.mat\n",
      "subj1_tr_9.mat\n",
      "subj1_tr_10.mat\n",
      "subj1_tr_11.mat\n",
      "subj1_tr_12.mat\n",
      "subj2\n",
      "subj2_tr_1.mat\n",
      "subj2_tr_2.mat\n",
      "subj2_tr_3.mat\n",
      "subj2_tr_4.mat\n",
      "subj2_tr_5.mat\n",
      "subj2_tr_6.mat\n",
      "subj2_tr_7.mat\n",
      "subj2_tr_8.mat\n",
      "subj2_tr_9.mat\n",
      "subj2_tr_10.mat\n",
      "subj2_tr_11.mat\n",
      "subj2_tr_12.mat\n",
      "20220816\n",
      "files listed.\n",
      "subj1\n",
      "subj1_tr_1.mat\n",
      "subj1_tr_2.mat\n",
      "subj1_tr_3.mat\n",
      "subj1_tr_4.mat\n",
      "subj1_tr_5.mat\n",
      "subj1_tr_6.mat\n",
      "subj1_tr_7.mat\n",
      "subj1_tr_8.mat\n",
      "subj1_tr_9.mat\n",
      "subj1_tr_10.mat\n",
      "subj1_tr_11.mat\n",
      "subj1_tr_12.mat\n",
      "subj2\n",
      "subj2_tr_1.mat\n",
      "subj2_tr_2.mat\n",
      "subj2_tr_3.mat\n",
      "subj2_tr_4.mat\n",
      "subj2_tr_5.mat\n",
      "subj2_tr_6.mat\n",
      "subj2_tr_7.mat\n",
      "subj2_tr_8.mat\n",
      "subj2_tr_9.mat\n",
      "subj2_tr_10.mat\n",
      "subj2_tr_11.mat\n",
      "subj2_tr_12.mat\n",
      "20221003\n",
      "files listed.\n",
      "subj1\n",
      "subj1_tr_1.mat\n",
      "subj1_tr_2.mat\n",
      "subj1_tr_3.mat\n",
      "subj1_tr_4.mat\n",
      "subj1_tr_5.mat\n",
      "subj1_tr_6.mat\n",
      "subj1_tr_7.mat\n",
      "subj1_tr_8.mat\n",
      "subj1_tr_9.mat\n",
      "subj1_tr_10.mat\n",
      "subj1_tr_11.mat\n",
      "subj1_tr_12.mat\n",
      "subj2\n",
      "subj2_tr_1.mat\n",
      "subj2_tr_2.mat\n",
      "subj2_tr_3.mat\n",
      "subj2_tr_4.mat\n",
      "subj2_tr_5.mat\n",
      "subj2_tr_6.mat\n",
      "subj2_tr_7.mat\n",
      "subj2_tr_8.mat\n",
      "subj2_tr_9.mat\n",
      "subj2_tr_10.mat\n",
      "subj2_tr_11.mat\n",
      "subj2_tr_12.mat\n",
      "2022100401\n",
      "files listed.\n",
      "subj1\n",
      "subj1_tr_1.mat\n",
      "subj1_tr_2.mat\n",
      "subj1_tr_3.mat\n",
      "subj1_tr_4.mat\n",
      "subj1_tr_5.mat\n",
      "subj1_tr_6.mat\n",
      "subj1_tr_7.mat\n",
      "subj1_tr_8.mat\n",
      "subj1_tr_9.mat\n",
      "subj1_tr_10.mat\n",
      "subj1_tr_11.mat\n",
      "subj1_tr_12.mat\n",
      "subj2\n",
      "subj2_tr_1.mat\n",
      "subj2_tr_2.mat\n",
      "subj2_tr_3.mat\n",
      "subj2_tr_4.mat\n",
      "subj2_tr_5.mat\n",
      "subj2_tr_6.mat\n",
      "subj2_tr_7.mat\n",
      "subj2_tr_8.mat\n",
      "subj2_tr_9.mat\n",
      "subj2_tr_10.mat\n",
      "subj2_tr_11.mat\n",
      "subj2_tr_12.mat\n",
      "2022100402\n",
      "files listed.\n",
      "subj1\n",
      "subj1_tr_1.mat\n",
      "subj1_tr_2.mat\n",
      "subj1_tr_3.mat\n",
      "subj1_tr_4.mat\n",
      "subj1_tr_5.mat\n",
      "subj1_tr_6.mat\n",
      "subj1_tr_7.mat\n",
      "subj1_tr_8.mat\n",
      "subj1_tr_9.mat\n",
      "subj1_tr_10.mat\n",
      "subj1_tr_11.mat\n",
      "subj1_tr_12.mat\n",
      "subj2\n",
      "subj2_tr_1.mat\n",
      "subj2_tr_2.mat\n",
      "subj2_tr_3.mat\n",
      "subj2_tr_4.mat\n",
      "subj2_tr_5.mat\n",
      "subj2_tr_6.mat\n",
      "subj2_tr_7.mat\n",
      "subj2_tr_8.mat\n",
      "subj2_tr_9.mat\n",
      "subj2_tr_10.mat\n",
      "subj2_tr_11.mat\n",
      "subj2_tr_12.mat\n",
      "20221005\n",
      "files listed.\n",
      "subj1\n",
      "subj1_tr_1.mat\n",
      "subj1_tr_2.mat\n",
      "subj1_tr_3.mat\n",
      "subj1_tr_4.mat\n",
      "subj1_tr_5.mat\n",
      "subj1_tr_6.mat\n",
      "subj1_tr_7.mat\n",
      "subj1_tr_8.mat\n",
      "subj1_tr_9.mat\n",
      "subj1_tr_10.mat\n",
      "subj1_tr_11.mat\n",
      "subj1_tr_12.mat\n",
      "subj2\n",
      "subj2_tr_1.mat\n",
      "subj2_tr_2.mat\n",
      "subj2_tr_3.mat\n",
      "subj2_tr_4.mat\n",
      "subj2_tr_5.mat\n",
      "subj2_tr_6.mat\n",
      "subj2_tr_7.mat\n",
      "subj2_tr_8.mat\n",
      "subj2_tr_9.mat\n",
      "subj2_tr_10.mat\n",
      "subj2_tr_11.mat\n",
      "subj2_tr_12.mat\n"
     ]
    }
   ],
   "source": [
    "# compute and save errors\n",
    "errors={}\n",
    "for folder in folders:\n",
    "    errors_folder={}   \n",
    "    print(folder)\n",
    "    files_dic = {}\n",
    "#    for folder in folders:\n",
    "#    folder_dic = {}\n",
    "    directory_path = '/ssd/zhibin/1overf/Cleaned_sourcedata/cortical_source_data/' + folder\n",
    "    files = list_files_in_directory(directory_path)\n",
    "\n",
    "    # load event time points\n",
    "    beha_filename = '/home/zhibinz2/Documents/GitHub/finger_tapping_behavioral_data/clean_' + str(folder) + '_bpchan.mat'\n",
    "    beh_data =  loadmat(beha_filename)\n",
    "    samples = beh_data['samples'][0]\n",
    "\n",
    "    for filename in files:\n",
    "        subject, rest = filename.split('_', 1)\n",
    "        if subject not in files_dic:\n",
    "            files_dic[subject] = {}\n",
    "        file = folder + '/' + filename\n",
    "        files_dic[subject][filename] = directory_path + '/' + filename\n",
    "    print('files listed.')\n",
    "\n",
    "    for subj,subject in enumerate(['subj1','subj2']):\n",
    "        # print(subj)\n",
    "        print(subject) \n",
    "        corr_matrices = {}\n",
    "        subject_files = list(files_dic[subject].keys())\n",
    "        subject_files = sorted(subject_files,key=sort_key) \n",
    "\n",
    "        for tr, file in enumerate(subject_files):\n",
    "            # print(tr)\n",
    "            print(file)\n",
    "            path = files_dic[subject][file]\n",
    "            filename = folder + '/' + file\n",
    "            # print(path)\n",
    "\n",
    "            # time_windows = segment_time_series(ts3, window_len, overlap_len, 200)\n",
    "            # segment data with tap-locked windows\n",
    "            tr_samples = samples[tr][:,subj]\n",
    "            time_windows = segment_time_series_taplocked(ts3, window_len, 2000,tr_samples)\n",
    "\n",
    "            # Compute difference(error) of the samples\n",
    "            if subject == 'subj1':\n",
    "                tr_samples_both = samples[tr]\n",
    "                num_tp=len(tr_samples_both)\n",
    "                tr_error=np.zeros(num_tp)\n",
    "                for tp in range(num_tp):\n",
    "                    tr_error[tp]=np.abs(tr_samples_both[tp][0]-tr_samples_both[tp][1])\n",
    "                errors_folder[file.split('_')[1]+(file.split('_')[2]).split('.')[0]]=tr_error\n",
    "    \n",
    "    errors[folder]=errors_folder\n",
    "                    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['subj2_tr_1.mat',\n",
       " 'subj2_tr_2.mat',\n",
       " 'subj2_tr_3.mat',\n",
       " 'subj2_tr_4.mat',\n",
       " 'subj2_tr_5.mat',\n",
       " 'subj2_tr_6.mat',\n",
       " 'subj2_tr_7.mat',\n",
       " 'subj2_tr_8.mat',\n",
       " 'subj2_tr_9.mat',\n",
       " 'subj2_tr_10.mat',\n",
       " 'subj2_tr_11.mat',\n",
       " 'subj2_tr_12.mat']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['tr1', 'tr2', 'tr3', 'tr4', 'tr5', 'tr6', 'tr7', 'tr8', 'tr9', 'tr10', 'tr11', 'tr12'])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors_folder.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([652., 550., 328., 244.,  97., 661., 686., 570., 459., 436., 336.,\n",
       "        96., 141., 516., 696., 644., 643., 210., 121., 473., 631., 664.,\n",
       "       715., 383.,  81., 185., 738., 499., 335.,  36., 253., 416., 739.,\n",
       "       482., 239., 364., 234., 345., 641., 597., 851., 666., 520., 492.,\n",
       "       561., 497., 573., 533., 559., 554., 509., 524., 500., 535., 647.,\n",
       "       707., 745., 720., 742., 814., 708., 615., 625., 463., 413., 852.,\n",
       "       551., 436., 344., 417., 501., 403., 488., 389., 303., 445., 546.,\n",
       "       513., 456., 473., 455., 452., 390., 325., 317., 358., 536., 425.,\n",
       "       222., 364., 473., 461., 485., 547., 611., 501., 505., 515., 489.,\n",
       "       525., 580., 494., 645., 781., 688., 680., 746., 800., 736., 748.,\n",
       "       655., 652., 558., 716., 609., 676., 578., 555., 453., 466., 651.,\n",
       "       614., 549., 494., 407., 681., 646., 677., 558., 644., 525., 581.,\n",
       "       382., 275., 196., 150., 271., 494., 563., 323., 318., 169., 101.,\n",
       "       355., 393., 326., 336., 470., 463., 434., 415., 593., 494., 381.,\n",
       "       399., 420., 359., 502., 399., 123., 157., 167., 103., 227., 173.,\n",
       "       403., 635., 566., 275., 147.,  43., 222., 100., 277., 238., 390.,\n",
       "       442., 276., 321.])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors_folder['tr1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([237., 200.,  73.,  72.,  61.,  35., 175.,  87., 124.,  74., 102.,\n",
       "       152., 205.,  75., 149.,  63.,  22.,  22.,  40.,  24.,  32.,  83.,\n",
       "       113.,  74.,  27.,   0.,  43.,  23., 103.,  73.,  51.,  45.,  61.,\n",
       "         3.,   8.,  23., 131., 108.,  87.,  13., 385., 353., 289., 244.,\n",
       "       333., 123.,  73.,  63.,  72.,  22.,   3.,  19.,   6.,  37.,  31.,\n",
       "        49.,  75.,  78.,  21.,  48.,  29.,  97., 109., 142.,  78., 321.,\n",
       "       203., 233.,  22.,   2., 169., 148.,  75.,  61.,  40.,  37.,  24.,\n",
       "        66.,  32.,  15.,  52.,  18., 134., 139., 131., 145.,  24.,  20.,\n",
       "        10., 103.,   1., 124.,   8.,  83., 142.,  70.,  71.,   2.,  93.,\n",
       "       141.,  53.,  43., 161., 185.,  48.,  60.,  95.,  69., 120., 212.,\n",
       "         2.,   3.,  53.,  53., 150., 236., 249., 185., 241., 219., 217.,\n",
       "       124.,  62., 233., 445., 211.,  94.,  28.,  34.,  64., 234., 194.,\n",
       "       185., 201., 302., 323., 453., 473., 198., 153., 168.,   9.,  93.,\n",
       "        11., 116.,  48., 216., 120., 245.,   8.,  31.,  64., 109., 195.,\n",
       "       142., 114., 165., 121., 274.,   7.,  93.,  68.,  59.,  57.,  27.,\n",
       "        58.,  17.,  31.,  61., 286., 324., 451., 171., 170., 251., 174.,\n",
       "        80., 137.,  37., 183., 151., 129., 171., 280., 169., 141.,  67.,\n",
       "       131., 178., 261., 176., 273., 155., 170., 146., 269., 304.])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors['20220713']['tr1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdf5storage import loadmat, savemat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "savemat('errors.mat',errors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdict=loadmat('errors.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([237., 200.,  73.,  72.,  61.,  35., 175.,  87., 124.,  74., 102.,\n",
       "       152., 205.,  75., 149.,  63.,  22.,  22.,  40.,  24.,  32.,  83.,\n",
       "       113.,  74.,  27.,   0.,  43.,  23., 103.,  73.,  51.,  45.,  61.,\n",
       "         3.,   8.,  23., 131., 108.,  87.,  13., 385., 353., 289., 244.,\n",
       "       333., 123.,  73.,  63.,  72.,  22.,   3.,  19.,   6.,  37.,  31.,\n",
       "        49.,  75.,  78.,  21.,  48.,  29.,  97., 109., 142.,  78., 321.,\n",
       "       203., 233.,  22.,   2., 169., 148.,  75.,  61.,  40.,  37.,  24.,\n",
       "        66.,  32.,  15.,  52.,  18., 134., 139., 131., 145.,  24.,  20.,\n",
       "        10., 103.,   1., 124.,   8.,  83., 142.,  70.,  71.,   2.,  93.,\n",
       "       141.,  53.,  43., 161., 185.,  48.,  60.,  95.,  69., 120., 212.,\n",
       "         2.,   3.,  53.,  53., 150., 236., 249., 185., 241., 219., 217.,\n",
       "       124.,  62., 233., 445., 211.,  94.,  28.,  34.,  64., 234., 194.,\n",
       "       185., 201., 302., 323., 453., 473., 198., 153., 168.,   9.,  93.,\n",
       "        11., 116.,  48., 216., 120., 245.,   8.,  31.,  64., 109., 195.,\n",
       "       142., 114., 165., 121., 274.,   7.,  93.,  68.,  59.,  57.,  27.,\n",
       "        58.,  17.,  31.,  61., 286., 324., 451., 171., 170., 251., 174.,\n",
       "        80., 137.,  37., 183., 151., 129., 171., 280., 169., 141.,  67.,\n",
       "       131., 178., 261., 176., 273., 155., 170., 146., 269., 304.])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outdict['20220713']['tr1']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eeg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
